{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "departmental-madrid",
   "metadata": {},
   "source": [
    "# Foreword\n",
    "\n",
    "For datasets large enough, that may not fit in RAM, Tensorflow's Data API makes it easy. You can create dataset, transform it, and tensorflow takes care of all the implementation details, such as multithreading, queuing, batching, prefetching, and so on. \n",
    "\n",
    "Data API can read from :\n",
    "- CSV file, text file\n",
    "- binary file\n",
    "- SQL databases,\n",
    "- Open source extensions available to read from all sorts of data sources. \n",
    "\n",
    "Data API performs :\n",
    "- Data Preprocessing\n",
    "- Transforms\n",
    "\n",
    "In this chapter, we will cover Data API, the TFRecord format and Feature API. \n",
    "\n",
    "\n",
    "## The Data API\n",
    "\n",
    "This represents a sequence of data items. Generally, we use datasets that gradually read data from disk, but for simplicity let's create a dataset entirely in RAM using ```tf.data.Dataset.from_tensor_slices(X)```. This function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X(along the first dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charming-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comprehensive-detective",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10) #any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "endless-trout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-safety",
   "metadata": {},
   "source": [
    "## Chaining Transformations\n",
    "Once you have dataset, you can apply all sorts of transformations to it by calling its transformation methods. Each method returns a new dataset, so you can chain transformations. \n",
    "- ```repeat()``` method repeats the dataset. so ```repeat(3)``` will change dataset size to 30.\n",
    "- ```batch(num)``` method will group the dataset in num sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hungry-juvenile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)\n",
    "    \n",
    "# if you use drop_remainder = true in batch method, last two tensor will be omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "european-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a new dataset with all the items doubled\n",
    "dataset = dataset.map(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "occupational-wrapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "focused-manual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 3 0 4 2 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 7 1 0 3 2 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 6 9 8 9 7 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 1 4 5 2 8 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([6 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# shuffling the data\n",
    "tf.random.set_seed(42)\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size = 3,seed = 42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-moore",
   "metadata": {},
   "source": [
    "### Working with California dataset (Split California Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acoustic-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(\n",
    "    housing.data,housing.target.reshape(-1,1),random_state = 42\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full,y_train_full,random_state = 42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-guitar",
   "metadata": {},
   "source": [
    "For a very large dataset that does not fit in memory, you will typically want to split it into many files first, then have Tensorflow read these files in parallel. To demonstrate this, let's start by splitting the housing dataset and save it to 20 CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sophisticated-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data,name_prefix,header = None,n_parts = 10):\n",
    "    housing_dir = os.path.join(\"datasets\",\"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok = True)\n",
    "    path_format = os.path.join(housing_dir,\"my_{}_{:02d}.csv\")\n",
    "    \n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx,row_indices in enumerate(np.array_split(np.arange(m),n_parts)):\n",
    "        part_csv = path_format.format(name_prefix,file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv,\"wt\",encoding = \"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nutritional-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train,y_train]\n",
    "valid_data = np.c_[X_valid,y_valid]\n",
    "test_data = np.c_[X_test,y_test]\n",
    "\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(\n",
    "    train_data,\"train\",header,n_parts = 20\n",
    ")\n",
    "valid_filepaths = save_to_multiple_csv_files(\n",
    "    valid_data,\"valid\",header,n_parts = 10\n",
    ")\n",
    "test_filepaths = save_to_multiple_csv_files(\n",
    "    test_data,\"test\",header,n_parts = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-blast",
   "metadata": {},
   "source": [
    "Okay, now let's take a peek at the first few lines of one of these CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "conceptual-fifty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-talent",
   "metadata": {},
   "source": [
    "## Building an Input Pipeline\n",
    "Let's suppose *train_filepaths* contains the list of file paths like shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "known-panel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/housing/my_train_00.csv',\n",
       " 'datasets/housing/my_train_01.csv',\n",
       " 'datasets/housing/my_train_02.csv',\n",
       " 'datasets/housing/my_train_03.csv',\n",
       " 'datasets/housing/my_train_04.csv',\n",
       " 'datasets/housing/my_train_05.csv',\n",
       " 'datasets/housing/my_train_06.csv',\n",
       " 'datasets/housing/my_train_07.csv',\n",
       " 'datasets/housing/my_train_08.csv',\n",
       " 'datasets/housing/my_train_09.csv',\n",
       " 'datasets/housing/my_train_10.csv',\n",
       " 'datasets/housing/my_train_11.csv',\n",
       " 'datasets/housing/my_train_12.csv',\n",
       " 'datasets/housing/my_train_13.csv',\n",
       " 'datasets/housing/my_train_14.csv',\n",
       " 'datasets/housing/my_train_15.csv',\n",
       " 'datasets/housing/my_train_16.csv',\n",
       " 'datasets/housing/my_train_17.csv',\n",
       " 'datasets/housing/my_train_18.csv',\n",
       " 'datasets/housing/my_train_19.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "important-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths,seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "laden-double",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-raleigh",
   "metadata": {},
   "source": [
    "Next, we can call ```interleave()``` method to read from 5 files at a time and interleave their lines (skipping the first line of each file, which is the header row, using the ```skip()``` method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "marine-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length = n_readers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-market",
   "metadata": {},
   "source": [
    "The ```interleave()``` method will create a dataset that will pull 5 file paths from the filepath_dataset, and for each one it will call the function we gave it ( a lambda in this example) to create a new dataset, in this case a *TextLineDataset*. It will then cycle through these 5 datasets, reading one line at a time from each until all datasets are out of items. Then it will get the next 5 file paths from the *filepath_dataset*, and interleave them the same way, and so on until it runs out of file paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "vanilla-metabolism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'\n",
      "b'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159'\n",
      "b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\n",
      "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
      "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-redhead",
   "metadata": {},
   "source": [
    "These are the first rows (ignoring header row) of 5 csv files, chosen randomly. But notice that these are just byte strings, we need to parse them, and also scale the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "front-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8  #X_train.shape[-1]\n",
    "\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([],dtype = tf.float32)]\n",
    "    fields = tf.io.decode_csv(line,record_defaults = defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bizarre-mixer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.39593136,  0.74167496, -0.16415128, -0.40340805, -0.61991787,\n",
       "        -0.18355484, -1.4084505 ,  1.2565969 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.504], dtype=float32)>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-science",
   "metadata": {},
   "source": [
    "- First, Mean and Standard deviation of each features were pre computed \n",
    "- The preprocess function takes one CSV line, and starts by parsing it. For this it uses the ```tf.io.csv_decode()``` function, which takes two argument: first - line to parse, second - array containing the default value for each column in csv file\n",
    "\n",
    "\n",
    "#### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "missing-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths,repeat = None,n_readers = 5,\n",
    "                      n_read_threads = None, shuffle_buffer_size = 10000,\n",
    "                      n_parse_threads = 5, batch_size = 32):\n",
    "    \n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length = n_readers, num_parallel_calls = n_read_threads\n",
    "    )\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-botswana",
   "metadata": {},
   "source": [
    "The above function ```csv_reader_dataset()``` is just the single function implementation of what we did above. There is one thing that is different i.e ```dataset.prefetch(1)```, which will do its best to create a batch of data ahead. In other word, when we are training single batch, another batch is ready to be trained. \n",
    "\n",
    "Let's implement this ```csv_reader_dataset()``` function to see how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "complex-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "X =  tf.Tensor(\n",
      "[[ 1.1832466  -0.2867314   0.256955   -0.0914653   0.6741611   0.05366582\n",
      "  -0.7432092   0.71184903]\n",
      " [-0.44522637  1.8491895  -0.32066625 -0.14044929 -0.10611927 -0.06691425\n",
      "  -0.691678    0.7318402 ]\n",
      " [ 0.3091969   0.5043504   0.20859428 -0.2770272   0.6084533   0.27369827\n",
      "  -0.84627515  0.7818199 ]], shape=(3, 8), dtype=float32)\n",
      "\r\n",
      "y =  tf.Tensor(\n",
      "[[3.151]\n",
      " [2.226]\n",
      " [2.141]], shape=(3, 1), dtype=float32)\n",
      "\r\n",
      "X =  tf.Tensor(\n",
      "[[-1.2879554   1.4536486  -0.5052248   0.20396037 -0.49580315  0.43515173\n",
      "  -0.7666345   0.6568782 ]\n",
      " [-0.64608806 -1.0778131  -0.35905546  0.09489206  1.0309911  -0.22977838\n",
      "  -0.72447133  0.9767287 ]\n",
      " [ 1.7620009  -0.6822723   0.7482188  -0.23329605 -0.6326944  -0.32895038\n",
      "  -1.3241241   1.1716374 ]], shape=(3, 8), dtype=float32)\n",
      "\r\n",
      "y =  tf.Tensor(\n",
      "[[1.141]\n",
      " [1.228]\n",
      " [3.923]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths,batch_size = 3)\n",
    "\n",
    "for X_batch, y_batch in train_set.take(2):\n",
    "    print(\"\\r\\nX = \",X_batch)\n",
    "    print(\"\\r\\ny = \",y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "floppy-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the dataset with keras.\n",
    "\n",
    "train_set = csv_reader_dataset(train_filepaths,repeat = None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "respective-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation = \"relu\",input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss = \"mse\", optimizer = keras.optimizers.SGD(lr = 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "artificial-pierre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 1.6673 - val_loss: 0.7947\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.6921 - val_loss: 0.6932\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.6194 - val_loss: 0.6581\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5927 - val_loss: 0.5646\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5848 - val_loss: 0.5157\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5374 - val_loss: 0.5809\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5425 - val_loss: 0.5223\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5046 - val_loss: 0.5885\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.4988 - val_loss: 0.5105\n",
      "Epoch 10/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.4836 - val_loss: 0.4388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6040096ac0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(train_set,steps_per_epoch = len(X_train) // batch_size,epochs = 10,\n",
    "         validation_data = valid_set,\n",
    "         validation_steps = len(X_valid) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "functional-wilson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4803062081336975"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set,steps = len(X_test)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-excitement",
   "metadata": {},
   "source": [
    "## The TFRecord Format\n",
    "The TFRecord format is Tensorflow's preferred format for storing large amount of data, such as images or audio, and reading it efficiently. It is a very simple binary format that just contains a sequence of binary records of varying sizes.\n",
    "\n",
    "Let's take a look at a simple example. \n",
    "- You can create a TFRecord file using ```tf.io.TFRecordWriter``` class\n",
    "- You can use ```tf.data.TFRecordDataset``` to read one or more TFRecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "impaired-collins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'This is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# First Write\n",
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"This is the second record\")\n",
    "\n",
    "# Next read\n",
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-pakistan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
