{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:,(2,3)] #petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) #iris setosa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_clf = Perceptron()\n",
    "per_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = per_clf.predict([[2,0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Perceptron for logical functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unit_step(v):\n",
    "    \"\"\" Heavyside Step Function. v must be scalar\"\"\"\n",
    "    if v >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def perceptron(x,w,b):\n",
    "    \"\"\" Function implemented by a perceptron with \n",
    "    weight vector w and bias b\"\"\"\n",
    "    \n",
    "    v = np.dot(w , x) + b\n",
    "    y = unit_step(v)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT(0) = 1\n",
      "NOT(1) = 0\n"
     ]
    }
   ],
   "source": [
    "def NOT_percept(x):\n",
    "    return perceptron(x,w = -1, b = 0.5)\n",
    "\n",
    "\n",
    "\"\"\" heavyside step function (- 1 * 0 + 0.5) == 1\"\"\"\n",
    "print(\"NOT(0) = {}\".format(NOT_percept(0)))\n",
    "\n",
    "\n",
    "\"\"\" heavyside step function (1 * -1 + 0.5) == 0\"\"\"\n",
    "print(\"NOT(1) = {}\".format(NOT_percept(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND Gate\n",
    "\n",
    "$y = \\theta(w_1 * x_1 + w_2 * x_2 + b)$\n",
    "\n",
    "Choose parameter $ w_1, w_2, b$ properly. \n",
    "$w_1 = 1, w_2 = 1, b = -1.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND(0, 0) = 0\n",
      "AND(0, 1) = 0\n",
      "AND(1, 0) = 0\n",
      "AND(1, 1) = 1\n"
     ]
    }
   ],
   "source": [
    "def AND_percept(x):\n",
    "    w = np.array([1,1])\n",
    "    b = -1.5\n",
    "    return perceptron(x,w,b)\n",
    "\n",
    "# Test\n",
    "example1 = np.array([0,0])\n",
    "example2 = np.array([0,1])\n",
    "example3 = np.array([1,0])\n",
    "example4 = np.array([1,1])\n",
    "\n",
    "print(\"AND({}, {}) = {}\".format(0, 0, AND_percept(example1)))\n",
    "print(\"AND({}, {}) = {}\".format(0, 1, AND_percept(example2)))\n",
    "print(\"AND({}, {}) = {}\".format(1, 0, AND_percept(example3)))\n",
    "print(\"AND({}, {}) = {}\".format(1, 1, AND_percept(example4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR gate\n",
    "\n",
    "$ w1 = 1, w2 = 1, b = -0.5 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR(0, 0) = 0\n",
      "OR(0, 1) = 1\n",
      "OR(1, 0) = 1\n",
      "OR(1, 1) = 1\n"
     ]
    }
   ],
   "source": [
    "def OR_percept(x):\n",
    "    w = np.array([1,1])\n",
    "    b = -0.5\n",
    "    return perceptron(x,w,b)\n",
    "\n",
    "# Test\n",
    "example1 = np.array([0,0])\n",
    "example2 = np.array([0,1])\n",
    "example3 = np.array([1,0])\n",
    "example4 = np.array([1,1])\n",
    "\n",
    "print(\"OR({}, {}) = {}\".format(0, 0, OR_percept(example1)))\n",
    "print(\"OR({}, {}) = {}\".format(0, 1, OR_percept(example2)))\n",
    "print(\"OR({}, {}) = {}\".format(1, 0, OR_percept(example3)))\n",
    "print(\"OR({}, {}) = {}\".format(1, 1, OR_percept(example4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Gate\n",
    "\n",
    "Xor gate can be made from combination of 3 gates mentioned above\n",
    "\n",
    "$ XOR(x_1,x_2) = AND(NOT(AND(x_1,x_2)), OR(x_1,x_2)) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR(0, 0) = 0\n",
      "XOR(0, 1) = 1\n",
      "XOR(1, 0) = 1\n",
      "XOR(1, 1) = 0\n"
     ]
    }
   ],
   "source": [
    "def XOR_net(x):\n",
    "    gate1 = AND_percept(x)\n",
    "    gate2 = OR_percept(x)\n",
    "    gate3 = NOT_percept(gate1)\n",
    "    \n",
    "    new_x = np.array([gate2,gate3])\n",
    "    \n",
    "    output = AND_percept(new_x)\n",
    "    return output\n",
    "    \n",
    "# Test\n",
    "example1 = np.array([0,0])\n",
    "example2 = np.array([0,1])\n",
    "example3 = np.array([1,0])\n",
    "example4 = np.array([1,1])\n",
    "\n",
    "    \n",
    "print(\"XOR({}, {}) = {}\".format(0, 0, XOR_net(example1)))\n",
    "print(\"XOR({}, {}) = {}\".format(0, 1, XOR_net(example2)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 0, XOR_net(example3)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 1, XOR_net(example4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result we're looking for. We combined 3 perceptrons above to get a more complex logical function. \n",
    "\n",
    "So, **is it possible to do the same like before (by finding the paraemters w and b?**. NO, because they don't exist. **WHY?**. The answer is that the XOR problem is not **Linearly Seperable**. \n",
    "\n",
    "\n",
    "**Hence, the limitation of perceptrons can be eliminated by stacking multiple perceptrons**. The resulting ANN is called **Multi-Layer Perceptron (MLP).** \n",
    "\n",
    "An MLP is composed of one (passthrough) input layer, one or more layers of TLUs, called hidden layers, and one final layer of TLUs called the output layers. \n",
    "\n",
    "When an ANN contains a deep stack of hidden layers, it is called **deep neural network(DNN)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training multi layer perceptron (ANN)\n",
    "\n",
    "-> trained using backward propagation algorithm.   \n",
    "\n",
    "-> performs gradient descent step. \n",
    "\n",
    "##### Algorithm in more detail:\n",
    "- handles one mini-batch at a time (for example 32 instances each) and goes through the full training set multiple times. \n",
    "\n",
    "- each mini-batch is passed to the network input layer, which sends it to hidden layer and so on. Hence, we get the output. This is known as **forward pass**. It is exactly like making predictions, except all intermediate results are preserved since they are needed for backward pass. \n",
    "\n",
    "- Next, algorithm measures the network's output error (using loss function).  Compares desired output with network output. \n",
    "\n",
    "- Then it computes how much each output connection contributed to the error. Done analytically by chain rule of calculus. It does so until it reaches input layer. \n",
    "\n",
    "- Finally, the algorithm performs gradient descent step to tweak all the connection weights.  \n",
    "\n",
    "\n",
    "In order for this algorithm to work properly, the authors made a key change to the MLP architecture's step function with the logistic sigmoid function. $\\sigma(z) = \\frac{1}{1 - \\exp(-z)}$. This was essential because the step function contains only the flat segments, so there is no gradient to work with, while logistic function has a well defined non zero derivative every wehere, allowing gradient descent to make progress at every step. \n",
    "\n",
    "\n",
    "#### Now we know where Neural net came from, what exactly we can do with them? \n",
    "\n",
    "#### 1) Regression MLPs:\n",
    "If you want to predict a single value (eg: price of house given many features) then you just need single output neuron. \n",
    "If you want to locate the center of an object on an image, you need to predict 2D coordinates, so you need two output neurons. .. and so on. \n",
    "\n",
    "In general, if you want to use MLPs for regression task, you should not use activation function for the output neurons, so they are free to output any range of values. \n",
    "\n",
    "#### 2) Classification MLPs:\n",
    "MLPs also does classification tasks. For binary classification, you just need one output neuron using the logistic activation function. You can also use it for multilabel binary classification. eg. predicting email is ham or spam, and urgent or non-urgent. For this you need two output neuron. \n",
    "\n",
    "You can also use it for multiclass classification. Eg: Classifying MNIST dataset, for 0 to 9 digit, you need 10 output neurons, and you should use softmax activation function for whole output layer. (Softmax activation function basically assigns probability for whole output neurons, s.t their sum altogether is 1)\n",
    "\n",
    "## Building an Image classifier using Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full,y_train_full),(X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "# every image is represented as 28 x 28 array rather than 1D array size of 784\n",
    "print(X_train_full.shape)\n",
    "print(X_train_full.dtype)\n",
    "# There are 60000 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set. and, since we are going to train using gradient descent, we must scale the input feature\n",
    "# For simplicity, divide by 255.0 to scale them in range 0-1.\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the class names for y are:\n",
    "class_names = [\"T-shirt/Top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\n",
    "              \"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle Boot\"]\n",
    "\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating model using sequential API\n",
    "\n",
    "# Sequential model -> simplest kind of keras model composed of single stack of layers connected sequentially.\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# first layer Flatten layer whose role is simply to convert each iput image to 1D array. Like X.reshape(-1,1)\n",
    "# Alternatively, you could also do -> model.add(keras.layers.InputLayer(shape = [28,28]))\n",
    "model.add(keras.layers.Flatten(input_shape = [28,28]))\n",
    "\n",
    "# Add Dense hidden layer with 300 neuron && 100 neuron, and it will use ReLU activation\n",
    "model.add(keras.layers.Dense(300,activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(100,activation = \"relu\"))\n",
    "\n",
    "#Finally, we add Dense output layer with 10 neuron (one per class) using softmax activation function.\n",
    "model.add(keras.layers.Dense(10,activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print model description\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense layer often has lot of parameters. For example first hidden layer has 784 x 300 parameters + 300 bias terms which adds up to 235,500. This gives the model quite a lot of flexibility to fit the training data, but also means model runs the risk of overfitting, especially when you donot have a lot of training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten\n",
      "dense\n",
      "dense_1\n",
      "dense_2\n"
     ]
    }
   ],
   "source": [
    "# print model layers. \n",
    "model.layers\n",
    "\n",
    "print(model.layers[0].name)\n",
    "print(model.layers[1].name)\n",
    "print(model.layers[2].name)\n",
    "print(model.layers[3].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the parameters of a layer can be accessed using its get_weights() and set_weights() method. For a dense layer, this includes both the connection weights and bias terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\r\n",
      " [[ 0.04958864 -0.04779727  0.00687333 ... -0.06269519  0.06484419\n",
      "   0.05254701]\n",
      " [ 0.04169325  0.04855789 -0.01027607 ...  0.04412204 -0.0452532\n",
      "  -0.06192764]\n",
      " [-0.04971872  0.05804987 -0.01706743 ... -0.013877   -0.04367473\n",
      "  -0.01593555]\n",
      " ...\n",
      " [ 0.00556499  0.03387158  0.01763874 ... -0.01227954 -0.04701785\n",
      "   0.0401414 ]\n",
      " [ 0.00731344 -0.05915475  0.0690787  ...  0.01081468  0.04263622\n",
      "   0.00148878]\n",
      " [-0.00412194 -0.07362765  0.03361884 ...  0.061204   -0.06167354\n",
      "   0.00611348]]\n",
      "\r\n",
      "\n",
      "Weight shape: (784, 300)\n"
     ]
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "weights,biases = hidden1.get_weights()\n",
    "print(\"Weights:\\r\\n\",weights)\n",
    "print(\"\\r\\n\")\n",
    "print(\"Weight shape:\",weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After a model is created, you must call its compile() method to specify the loss function and the optimizer\n",
    "# to use. \n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "             optimizer = \"sgd\",\n",
    "             metrics = [\"accuracy\"])\n",
    "\n",
    "# we used sgd optimizer i.e train the model using sgd. \n",
    "# Finally since this is classifier, it's useful to measure its \"accuracy\" during training and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "# Although there are 55000 data, in epochs below, 1719 images are processed. This is because the default batch\n",
    "# size is 32. i.e mean loss of 32 images are calculated at once. and weights are adjusted. \n",
    "# 55000 / 32 = 1719"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7344 - accuracy: 0.7588 - val_loss: 0.5360 - val_accuracy: 0.8096\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4937 - accuracy: 0.8281 - val_loss: 0.4572 - val_accuracy: 0.8434\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4482 - accuracy: 0.8441 - val_loss: 0.4203 - val_accuracy: 0.8554\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4195 - accuracy: 0.8525 - val_loss: 0.4061 - val_accuracy: 0.8602\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3991 - accuracy: 0.8618 - val_loss: 0.3971 - val_accuracy: 0.8620\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3841 - accuracy: 0.8643 - val_loss: 0.3725 - val_accuracy: 0.8716\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3692 - accuracy: 0.8711 - val_loss: 0.3627 - val_accuracy: 0.8778\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3574 - accuracy: 0.8742 - val_loss: 0.3576 - val_accuracy: 0.8754\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3476 - accuracy: 0.8769 - val_loss: 0.3491 - val_accuracy: 0.8782\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3380 - accuracy: 0.8802 - val_loss: 0.3432 - val_accuracy: 0.8808\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3288 - accuracy: 0.8836 - val_loss: 0.3511 - val_accuracy: 0.8740\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3207 - accuracy: 0.8859 - val_loss: 0.3386 - val_accuracy: 0.8800\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3129 - accuracy: 0.8893 - val_loss: 0.3525 - val_accuracy: 0.8714\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3072 - accuracy: 0.8903 - val_loss: 0.3291 - val_accuracy: 0.8830\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3013 - accuracy: 0.8922 - val_loss: 0.3231 - val_accuracy: 0.8848\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2939 - accuracy: 0.8933 - val_loss: 0.3234 - val_accuracy: 0.8830\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2877 - accuracy: 0.8969 - val_loss: 0.3089 - val_accuracy: 0.8882\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2816 - accuracy: 0.9001 - val_loss: 0.3116 - val_accuracy: 0.8876\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2767 - accuracy: 0.9017 - val_loss: 0.3059 - val_accuracy: 0.8876\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2719 - accuracy: 0.9026 - val_loss: 0.3022 - val_accuracy: 0.8912\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2664 - accuracy: 0.9038 - val_loss: 0.3072 - val_accuracy: 0.8888\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2617 - accuracy: 0.9052 - val_loss: 0.2977 - val_accuracy: 0.8914\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2571 - accuracy: 0.9073 - val_loss: 0.3062 - val_accuracy: 0.8886\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2528 - accuracy: 0.9088 - val_loss: 0.3089 - val_accuracy: 0.8878\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2483 - accuracy: 0.9111 - val_loss: 0.3212 - val_accuracy: 0.8806\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2448 - accuracy: 0.9124 - val_loss: 0.2920 - val_accuracy: 0.8924\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2404 - accuracy: 0.9138 - val_loss: 0.2893 - val_accuracy: 0.8950\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2360 - accuracy: 0.9157 - val_loss: 0.2969 - val_accuracy: 0.8940\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2319 - accuracy: 0.9181 - val_loss: 0.3084 - val_accuracy: 0.8884\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2285 - accuracy: 0.9197 - val_loss: 0.2930 - val_accuracy: 0.8964\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluating model\n",
    "\n",
    "history = model.fit(X_train,y_train,epochs = 30,\n",
    "                   validation_data = (X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit() method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch) and most importantly a dictionary, (history.history) containing the loss and extra metrics it measured at the end of each epoch on training set and on the validation set (if any). \n",
    "\n",
    "If you create a pandas dataframe, you can call its plot() method to get learning curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize = (8,5))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 62.7417 - accuracy: 0.8551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[62.74165725708008, 0.8550999760627747]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using model to make predictions\n",
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-81ace37e545f>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle Boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Regression MLP using Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing.data,housing.target,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_full,y_train_full,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.fit_transform(X_valid)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation = \"relu\",input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "# 1 epoch = 11610 / 32 = 363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.6419 - val_loss: 0.8077\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7047 - val_loss: 0.6736\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6345 - val_loss: 0.6243\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 997us/step - loss: 0.5977 - val_loss: 0.5977\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5706 - val_loss: 0.5708\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5472 - val_loss: 0.5538\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5288 - val_loss: 0.5370\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5130 - val_loss: 0.5257\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4992 - val_loss: 0.5142\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4875 - val_loss: 0.5040\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4777 - val_loss: 0.4977\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4688 - val_loss: 0.4894\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4615 - val_loss: 0.4848\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4547 - val_loss: 0.4804\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4488 - val_loss: 0.4747\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4435 - val_loss: 0.4713\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4389 - val_loss: 0.4672\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4347 - val_loss: 0.4642\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4629\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 0.4603\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train_scaled,y_train,epochs = 20, validation_data = (X_valid_scaled,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 807us/step - loss: 0.4190\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32761034]\n",
      " [1.7185787 ]\n",
      " [3.3415675 ]]\n"
     ]
    }
   ],
   "source": [
    "X_new = X_test_scaled[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, sequential API is quite easy to use. However, although sequential models are extremely common, it is sometimes useful to build neural networks with more complex topologies, or with multiple input outputs. For this purpose, keras offers functional APIs\n",
    "\n",
    "## Building Complex Models Using the Functional API\n",
    "\n",
    "One example of non-sequential neural network is *Wide and Deep neural network*. It connects all or part of inputs directly to the output later. This architecture makes it possible for the neural network to learn both deep patterns (using deep path) and simple rules (through short path). Let's build such neural network to tackle the housing dataset above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing.data,housing.target,random_state = 42)\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_full,y_train_full,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.fit_transform(X_valid)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First creating input object. This is needed because we may have multiple inputs\n",
    "input = keras.layers.Input(shape = X_train.shape[1:])\n",
    "\n",
    "# Dense layer with 30 neurons. As soon as it is created, we pass input. This is functional API\n",
    "hidden1 = keras.layers.Dense(30,activation = 'relu')(input)\n",
    "\n",
    "# 2nd Dense layer with 30 neurons. again use it as function\n",
    "hidden2 = keras.layers.Dense(30,activation = \"relu\")(hidden1)\n",
    "\n",
    "# concatenate layer to concatenate input and output of the second hidden layer. \n",
    "concat = keras.layers.Concatenate()([input,hidden2])\n",
    "\n",
    "# output layer with single  neuron \n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "\n",
    "model = keras.models.Model(inputs = [input],outputs = [output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 30)           270         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 30)           930         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 38)           0           input_2[0][0]                    \n",
      "                                                                 dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            39          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.2611 - val_loss: 0.7115\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6580 - val_loss: 0.6351\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5878 - val_loss: 0.5926\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5582 - val_loss: 0.5727\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5347 - val_loss: 0.5501\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5158 - val_loss: 0.5337\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5002 - val_loss: 0.5209\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4876 - val_loss: 0.5110\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4760 - val_loss: 0.4997\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4659 - val_loss: 0.4907\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4577 - val_loss: 0.4855\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4498 - val_loss: 0.4779\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4428 - val_loss: 0.4746\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4366 - val_loss: 0.4693\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4307 - val_loss: 0.4644\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4257 - val_loss: 0.4594\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4210 - val_loss: 0.4563\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4167 - val_loss: 0.4534\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4121 - val_loss: 0.4515\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4088 - val_loss: 0.4499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fae9fbe8be0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = \"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "model.fit(X_train_scaled,y_train,epochs = 20, validation_data = (X_valid_scaled,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 794us/step - loss: 0.4122\n",
      "[[0.3919998]\n",
      " [1.9343274]\n",
      " [3.6921172]]\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test_scaled,y_test)\n",
    "X_new = X_test_scaled[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Input\n",
    "What if you want to send different subsets of input features through the wide or deep paths? We will send 5 features (features 0 to 4) through wide path, and 6 features through the deep path. (features 2 to 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape = [5],name = \"wide_input\")\n",
    "input_B = keras.layers.Input(shape = [6],name = \"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30,activation = 'relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30,activation = 'relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A,hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "model = keras.models.Model(inputs=[input_A,input_B],outputs = [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "deep_input (InputLayer)         [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 30)           210         deep_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "wide_input (InputLayer)         [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 30)           930         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 35)           0           wide_input[0][0]                 \n",
      "                                                                 dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1)            36          concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,176\n",
      "Trainable params: 1,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"mse\",optimizer = \"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to pass through model\n",
    "X_train_A,X_train_B = X_train_scaled[:,:5],X_train_scaled[:,2:]\n",
    "X_valid_A,X_valid_B = X_valid_scaled[:,:5],X_valid_scaled[:,2:]\n",
    "X_test_A,X_test_B = X_test_scaled[:,:5],X_test_scaled[:,2:]\n",
    "X_new_A,X_new_B = X_test_A[:3],X_test_B[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7335 - val_loss: 0.5095\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4801 - val_loss: 0.4708\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4519 - val_loss: 0.4803\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4283 - val_loss: 0.7056\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.4510\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4039 - val_loss: 0.5124\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4086 - val_loss: 0.4503\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3918 - val_loss: 0.4559\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3857 - val_loss: 0.4527\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3825 - val_loss: 0.4451\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3865 - val_loss: 0.4597\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.4489\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3785 - val_loss: 0.4464\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3712 - val_loss: 0.4457\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3724 - val_loss: 0.4448\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3661 - val_loss: 0.4464\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.4486\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3620 - val_loss: 0.4429\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3679 - val_loss: 0.4485\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3604 - val_loss: 0.4399\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_A,X_train_B),y_train,epochs = 20,\n",
    "                    validation_data = ((X_valid_A,X_valid_B),y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD7CAYAAABkO19ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6qElEQVR4nO3deXzcdZ348dc7M5ncySRpmzZJ75bSg7Ol5bIUuaq41AO1oiysq9WH4vlTwZ+76OL6W911vRZcRRY8sai7QheLCNLIIT2h0Iu2oUnvM0mTTps7798fn5l0mmaSSebs5P18POYxM9/zPdP0/f3M5/s5RFUxxhiTubJSHYAxxpjEskRvjDEZzhK9McZkOEv0xhiT4SzRG2NMhrNEb4wxGS6qRC8ii0Vku4jUisg9/az/rohsDD52iMjxsHXdYetWxDF2Y4wxUZDB2tGLiAfYAdwA7APWAR9Q1a0Rtv8UcImqfjj4PqCqhXGN2hhjTNS8UWwzH6hV1V0AIrIcWAL0m+iBDwBfHW5Ao0aN0kmTJg13d06ePElBQcGw9080iy82Fl9sLL7YpHN8GzZsOKaqo/tdqaoDPoBbgYfC3t8O3B9h24nAQcATtqwLWA+sBt452Pnmzp2rsVi1alVM+yeaxRcbiy82Fl9s0jk+YL1GyKvRVN3cCixW1Y8E398OLFDVu/rZ9m6gWlU/FbasSlX3i8gU4DngOlV9s89+y4BlABUVFXOXL18+YEwDCQQCFBamb02RxRcbiy82Fl9s0jm+a6+9doOqzut3ZaQrgJ4ukV8BPB32/svAlyNs+ypw5QDH+ilw60DnsxJ9all8sbH4YmPxDR8DlOijaXWzDpguIpNFxAcsBc5qPSMi5wOlwMthy0pFJCf4ehRwFZHr9o0xxiTAoDdjVbVLRO4CngY8wMOqukVE7sNdQUJJfymwPHhlCZkJ/FhEenBNOb+pEVrrGGOMSYxoWt2gqiuBlX2W3dvn/df62e+vwAUxxGeMMSZG1jPWGGMynCV6Y4zJcBmT6JtbO/neszvY1dyd6lCMMSatZEyiF4HvPbuT7Y09qQ7FGGPSSsYk+uLcbIpyvRxrtURvjDHhMibRA1T582hss8nOjTEmXMYl+mOtluiNMSZcRiX6Sn8ejW1WdWOMMeEyKtFXleZxshMC7V2pDsUYY9JGRiX6Sn8eAAeOt6Y4EmOMSR8Zleirgol+f5MlemOMCcnMRG8lemOM6ZVRiX5MUQ4esURvjDHhMirRZ2UJZbliVTfGGBMmoxI9QHme2M1YY4wJk3mJPjfLqm6MMSZM5iX6POFwSxud3dZxyhhjIEMTfY/Coea2VIcyNKrw7Nfg0OZUR2KMyTBRJXoRWSwi20WkVkTu6Wf9d0VkY/CxQ0SOh627Q0R2Bh93xDH2fpXnuo90ztXTBw7Di9+FTb9NdSTGmAwz6JyxIuIBHgBuAPYB60RkRfgk36r6ubDtPwVcEnxdBnwVmAcosCG4b1NcP0WYUXkCnINNLJvq3XPz3pSGYYzJPNGU6OcDtaq6S1U7gOXAkgG2/wDw6+Drm4BnVLUxmNyfARbHEvBgynJdoj/nSvSNde65eV9q4zDGZJxBS/RAFRBezNwHLOhvQxGZCEwGnhtg36p+9lsGLAOoqKigpqYmirD619F6kmKfsG7rLmqy9g/7OIkSCAT6/XyT6mqYBLQfruXlGD5/rCLFly4svthYfLFJ9/giiSbRD8VS4HeqOqSJW1X1QeBBgHnz5umiRYuGHUBNTQ2TxnjRfB+LFs0f9nESpaamhn4/X6P7EZTT0ciit1wFnuzkBhYUMb40YfHFxuKLTbrHF0k0VTf7gfFh76uDy/qzlNPVNkPdN26q/HnsbzqV6NPEV1Ow6gaFlgMpDcUYk1miSfTrgOkiMllEfLhkvqLvRiJyPlAKvBy2+GngRhEpFZFS4MbgsoSq8udx4HgbqufQbFNN9VASvCbaDVljTBwNmuhVtQu4C5egtwG/UdUtInKfiNwStulSYLmGZVdVbQS+jrtYrAPuCy5LqEp/Hq2d3TSd6kz0qeKj45RrXjnpLe693ZA1xsRRVHX0qroSWNln2b193n8twr4PAw8PM75hqSo9PS59WYEvmacenlDTyklXwWuPwnEr0Rtj4ifjesbCOTgufSjRj54JBaOt6sYYE1eW6NNBKNGXTnL19FZ1Y4yJo4xM9P78bPKyPedOp6mmOsgphvwyKKm2Er0xJq4yMtGLCFWleefOBCRN9VA6EUROl+jPpRZDxpi0lpGJHlzLmwPN51Kin+Re+8dD5yk4lfDGScaYESJjE73rNHUOJPqeHmjaDaWT3fuSavds1TfGmDjJ4ESfS8PJDto6hzQaQ/KdOAjd7adL9L2dpuyGrDEmPjI30ZeeIy1vwlvcgPWONcbEXcYm+soSl+jTvuVNaIybsmDVTX4ZePOsRG+MiZuMTfThvWPTWlM9iOd0SV7E3ZA9vielYRljMkfGJvqxxblkyTlSdVNSfeawxCXVVqI3xsRNxiZ6ryeLscW56Z/oG+tO18+HWO9YY0wcZWyiB86NTlPhbehDSsbDySPQ2ZaKiIwxGSajE33ad5pqPwGnjp2+ERsSakvfkn5TIRpjzj0Zneir/HkcPN5Gd0+aDifQt2lliD94Y9ZuyBpj4iCjE32lP4+uHuXoifZUh9K/SIm+t3es1dMbY2KX0Yn+dKepNJ0/tjHYhr60T9VNcRUgluiNMXERVaIXkcUisl1EakXkngjbvE9EtorIFhF5NGx5t4hsDD7Omms2kU6PS5+mNzWb6iHXD3n+M5d7sqFonPWONcbExaBTCYqIB3gAuAHYB6wTkRWqujVsm+nAl4GrVLVJRMaEHaJVVS+Ob9jRqfSneaep/lrchNi49MaYOImmRD8fqFXVXaraASwHlvTZ5qPAA6raBKCqR+Ib5vAU5ngpyctO32EQmurObnET4h9vc8caY+IimsnBq4DwjLMPWNBnm/MAROQlwAN8TVX/GFyXKyLrgS7gm6r6eN8TiMgyYBlARUUFNTU1Q/gIZwoEAmfsX+Lt5vU391FTc2zYx4yn3vi0m4VNu9lbeAl1/XzeKc1K9fG9PL/qOZDk3Urp+/2lG4svNhZfbNI9vohUdcAHcCvwUNj724H7+2zzJPB7IBuYjLsw+IPrqoLPU4B6YOpA55s7d67GYtWqVWe8/8jP1umN3/lLTMeMp974mnarfrVYdf0j/W+45kG3vuVQskJT1bO/v3Rj8cXG4otNOscHrNcIeTWaouJ+YHzY++rgsnD7gBWq2qmqdcAOYHrwQrI/+LwLqAEuif4yFLsqf156Vt1EanETYsMVG2PiJJpEvw6YLiKTRcQHLAX6tp55HFgEICKjcFU5u0SkVERywpZfBWwliar8eZxo76K5tTOZpx1cpDb0ITbTlDEmTgZN9KraBdwFPA1sA36jqltE5D4RuSW42dNAg4hsBVYBX1TVBmAmsF5EXgsu/6aGtdZJhlDLm7Qr1TfVQ5Y32Ga+H729Yy3RG2NiE83NWFR1JbCyz7J7w14r8PngI3ybvwIXxB7m8IWPSz9zXHEqQzlTUx34J4Anwj9BbgnkFFunKWNMzDK6ZyxApT8XIP0GNxuoDX1IyXirujHGxCzjE/2oghx83qz06zTVVB/5RmyIdZoyxsRBxif6rCyhsiTNJiBpPQ6tTVGU6G2mKWNM7DI+0UNwApJ0SvSDtbgJ8Y93F4T2QKIjMsZksBGR6CtL0qwtfSjRRxr+IKS3Lb2V6o0xwzciEn1VaR5HTrTT0dWT6lCcpmBnKf/EgbezTlPGmDgYEYm+0p+HKhxMl5Y3TfWQXw65gzT3tE5Txpg4GBGJvrp3XPo0SfSNdYO3uAEoGus6VVnVjTEmBiMi0Yd3mkoL0bShB8jyQHGl9Y41xsRkRCT6sSXBTlNpMNOU9HS5Eno0iR6CnaasRG+MGb4RkehzvB7GFOWkxdyxOe1HQbsHb3ETYr1jjTExGhGJHtwN2XQo0ee1HnYvoi7RV0PLAejuSlhMxpjMNmISfbp0msptO+ReDCXRazcEDiUsJmNMZhs5id7vEr0baDN18loPgccHRZXR7WDDFRtjYjSiEn1HVw/HAh0pjSOv9ZDrKJUV5VdvvWONMTEaMYk+XSYgyW07HH21DYR1mtqTkHiMMZlvxCT6qnToNKXqSvTRtrgB8BVAXpmV6I0xwzbyEn0qO021NuHtPjW0Ej3YcMXGmJhElehFZLGIbBeRWhG5J8I27xORrSKyRUQeDVt+h4jsDD7uiFfgQ1Wc56Uwx5vaEn1oMLNohj8I559gN2ONMcM26JyxIuIBHgBuAPYB60RkRfgk3yIyHfgycJWqNonImODyMuCrwDxAgQ3BfZvi/1EG/Ry9LW9SpjGU6CcNbb+SathVA6ogEu+ojDEZLpoS/XygVlV3qWoHsBxY0mebjwIPhBK4qh4JLr8JeEZVG4PrngEWxyf0oav056b2ZmzvhCODDE/cV8l46AhA2/F4R2SMGQEGLdEDVUB4vcE+YEGfbc4DEJGXAA/wNVX9Y4R9q/qeQESWAcsAKioqqKmpiTL8swUCgYj7y6l26o92xXT8WMx442VKs/2s/uu6Ie03+kgzs4F1f36ck4VDrPYZooG+v3Rg8cXG4otNuscXSTSJPtrjTAcWAdXA8yJyQbQ7q+qDwIMA8+bN00WLFg07kJqaGiLtv5Vantu7ncuuuJqCnHh99CGo/zbNeWMjxhfRviLY+q9cNn0szBjivkM00PeXDiy+2Fh8sUn3+CKJpupmPzA+7H11cFm4fcAKVe1U1TpgBy7xR7Nv0lSlui19Uz2teWOHvp/1jjXGxCCaRL8OmC4ik0XEBywFVvTZ5nFcaR4RGYWrytkFPA3cKCKlIlIK3BhclhIpbUvf1QHN+2jLHUaizx8FnhwbxdIYMyyD1l+oapeI3IVL0B7gYVXdIiL3AetVdQWnE/pWoBv4oqo2AIjI13EXC4D7VLUxER8kGr0TkKQi0R/fAyiteRVD3zcrK9iW3hK9MWbooqqoVtWVwMo+y+4Ne63A54OPvvs+DDwcW5jxMaYoF2+WpKbqJtjiZlglerBOU8aYYRsxPWMBPFnC2JLc1PSODXaWGlYdPdhMU8aYYRtRiR7c4GYpqbppqgdvLh2+0uHt7x8PJw65un5jjBmCEZfoq1M101RoQvDh9mwtqQYUWlLWaMkYc44acYm+0p/HoZY2urp7knvixrqhj3ETrndcersha4wZmhGX6KtK8+juUQ6faE/eSVVPl+iHq3dcequnN8YMzchL9KkYrvjkMeg8aYneGJMSIy7Rp2SmqdDwxEOZcKQvbw4UVgTb4xtjTPRGXKJPSe/Y3lErJ8V2HGtLb4wZhhGX6PN8HsoKfKlJ9P4hDk/cV8l4uxlrjBmyEZfowZXqk1pH31gHRZWQnRvbcUIletX4xGWMGRFGZKJP+gQksba4CfFPgK42d3PXGGOiNCITfZU/n/3HW9FklYyb6mO7ERvS2/LGqm+MMdEbkYm+0p/LqY5umls7E3+yzlY4cSA+JXprYmmMGYYRmeirg8MV70tGPX2oOWRcEr31jjXGDN2ITPSVyWxi2du0Mg5VN3mlkF1gJXpjzJCMyESf1CkFG4OdpeJRohdxo1hapyljzBCMyERfVuAjNzsrOU0sm+pdKbxgVHyOZ52mjDFDFFWiF5HFIrJdRGpF5J5+1t8pIkdFZGPw8ZGwdd1hy/vONZsSIkKlP48DzUlK9GWThz88cV+W6I0xQzToVIIi4gEeAG4A9gHrRGSFqm7ts+ljqnpXP4doVdWLY440zpLWaaqpDsqnxe94JePh1DHoOAW+/Pgd1xiTsaIp0c8HalV1l6p2AMuBJYkNK/Gq/HnsT/QEJPEYnrivUMsbm4DEGBOlaCYHrwLC2/PtAxb0s917RGQhsAP4nKqG9skVkfVAF/BNVX28744isgxYBlBRUUFNTU3UH6CvQCAQ1f4dxzs4FujkT39ehc8Tp2qVPnztjVzZ1caOY50cCMYUbXyRlBw/xiXAa88/SVPZJXGJM1ys8SWaxRcbiy826R5fRKo64AO4FXgo7P3twP19tikHcoKvPwY8F7auKvg8BagHpg50vrlz52osVq1aFdV2v1u/Vyfe/aTuOhqI6XwDqv+r6leLVXc807so2vgiatrtjrn+p7EdJ4KY40swiy82Fl9s0jk+YL1GyKvRVN3sB8aHva8OLgu/WDSoamjKpoeAuWHr9gefdwE1QPyLocNQVZqECUjiNTxxuKJxIFl2Q9YYE7VoEv06YLqITBYRH7AUOKP1jIiMC3t7C7AtuLxURHKCr0cBVwF9b+KmRFLa0jfVA+IGI4sXT7YbCdN6xxpjojRoHb2qdonIXcDTgAd4WFW3iMh9uJ8KK4BPi8gtuHr4RuDO4O4zgR+LSA/uovJNPbu1TkqMLclFBPYlNNHXueaQXl98j2tNLI0xQxDNzVhUdSWwss+ye8Nefxn4cj/7/RW4IMYYEyLbk0VFUW7iq27iWW0T4h8Pe9fG/7jGmIw0InvGhlSV5iW26qaxLjGJvqQaWg5AT3f8j22MyTgjOtFX+vMSN7BZx0k4eSRxib6nEwJH4n9sY0zGGdGJvsqfx8HmVnp6EjABSdNu9xyPCUf6Kgne3LUbssaYKIzsRF+aR2e3cjTQPvjGQ9UUx1Er+7KZpowxQzCyE73fTdadkOqbeI5D31co0R+3RG+MGdwIT/RuULCEtLxpqoecEjdZSLzlFkNuiTWxNMZEZUQn+spgiT4hLW8a66B0YvyGJ+6rZLwlemNMVEZ0oi/KzaY415u4qptE1M+HlIy3OnpjTFRGdKIH18Qy7iX6nh44vjsxLW5CSqot0RtjojLiE311aR774l1Hf+IAdHcktkTvHw9tzdDWkrhzGGMywohP9AnpNJWIUSv76m1iafX0xpiBjfhEX+XP40RbFy1tnfE7aCKbVoaEZpqyRG+MGcSIT/SViRiuuLEOxHO61J0IvYl+T+LOYYzJCCM+0YcmIIlrom+qd0nekx2/Y/ZVWAFZ2fEt0R94FU/XyfgdzxiTFjIr0Xd3DXmXan8CZppqqk9sixuArCwoqYpf79iGN+En13Hejh/F53jGmLSROYn++F740dWUNWwY0m6jCnPwebLYf7wtfrE0JWh44r7i2Wnqxe+AdjPmyIsu6RtjMkbmJPr8csjyMnPbd1wdeZSysoRx/tz4tbxpa4FTDYm9ERsSr5mmmurhteUw5z30ZHnhhX+P/ZjGmLSROYnelw/v/zmg8JvboTP6xF1ZEsdOU8eDwxMnq0R/4gB0x9hi6MXvugnHb/xnDo67ySX90DDLxphzXlSJXkQWi8h2EakVkXv6WX+niBwVkY3Bx0fC1t0hIjuDjzviGfxZyqawbebn4dAmePLzoNGNM19Vmhe/OvrGBA5P3FdJNWgPnDg4/GMc3wuv/gou/VsormTPhHdBlsclf2NMRhg00YuIB3gAeBswC/iAiMzqZ9PHVPXi4OOh4L5lwFeBBcB84KsikoDhHE9rLJ8H19wDrz0KGx6Jap9Kfx6HT7TR2d0TewChNvSJvhkLrncsxHZD9qXvueerPgtAR045XPIh2PgraN4fU3jGmPQQTYl+PlCrqrtUtQNYDiyJ8vg3Ac+oaqOqNgHPAIuHF+oQXHM3TLsBVn4J9q0fdPNqfx6qcKg5Djdkm+rc0MS5JbEfazCxdppqOQCv/Bwu+eDpiwbA1Z9zvxRe+n7sMRpjUs4bxTZVQHiRcR+uhN7Xe0RkIbAD+Jyq7o2wb1XfHUVkGbAMoKKigpqamqiC708gEKDm+efxVtzB3H2vI794HxvmfodOnz/iPkePuUm2n1z1MjPLPcM+N8CFb76C11vOKxE+QyAQiOnzhcvqbmchsGvj8+xpqhjy/tN2PkhlTzdrvVfSFowpEAhQs3EXM8YsYsz6R1jjuYKOnIT+CBuSeH5/iWDxxcbiSxBVHfAB3Ao8FPb+duD+PtuUAznB1x8Dngu+/gLwD2Hb/SPwhYHON3fuXI3FqlWrTr85sFH162NUH7lZtasz4j67jgZ04t1P6u/W743p3Kqq+v2LVX9zZ3TxxcO3pqiu+PTQ92s5qHrfaNXHP3HG4t74jtWqfs2v+vRXYo8xjuL+/cWZxRcbi2/4gPUaIa9GU3WzHwj7XU91cFn4xaJBVUMTrz4EzI1234QadxG847tQ/wI89/XIm5XEaUrB7i44vic5N2JDhtvE8qUfQE8XvOX/9L++fCrMuRXWPQwnG2KL0RiTUtEk+nXAdBGZLCI+YCmwInwDERkX9vYWYFvw9dPAjSJSGrwJe2NwWfJcfBvM+7C76bh1Rb+b5GZ7GFWYE3sTy5b9LnkmM9H7xw/9ZmzgCKx/GC58H5RNibzdwi9A5ylY/UBsMRpjUmrQRK+qXcBduAS9DfiNqm4RkftE5JbgZp8WkS0i8hrwaeDO4L6NwNdxF4t1wH3BZcm1+JtQNQ8e/wQc3dHvJlXx6DSVzBY3IaHesVE2JQXgr/8B3e2RS/Mho2fArCWw5kFobYotTmNMykTVjl5VV6rqeao6VVW/EVx2r6quCL7+sqrOVtWLVPVaVX0jbN+HVXVa8BFde8d48+bA+37unh/7ELQHztqkqjQO49I3JbENfUhJNXSejD4RnzwG6x6COe+BUdMH337hF6HjhEv2xphzUub0jB1MSRXc+jA07IQVd51VAq4KTimoQykZ99VU70aULD6rYVHi9DaxjLL65uUHXK/hhV+Mbvuxc2DG22H1D202K2POUSMn0QNMuQau+yps+b1LeGEq/Xm0dfbQeLJj+Mdvqgf/BNezNFmGMtPUqUZY+yDMfperlonWwi9C23H3S8AYc84ZWYke4KrPwMy/gWfuhfoXexdXhYYrjqX6pjFJo1aG809wz9HckF39n9ARiL40H1J1KUy7Hl6+HzpsvHpjzjUjL9GLwJIfutYmv73T9Q4lDjNNdZxK3vDE4fLLwZs7eNVN63FY8yOYeQtU9DeCxSAWfsmNyrnhp8OJ0hiTQiMv0QPkFsP7f+mS82/vhK4OJpTn480SHn6pnkB7lBOYnGxwA4L9+jb41ynQ1jy8JBoLkWBb+kES/ZofQ3vL0EvzIRMWwOSFrv19ZxzH7jfGJNzITPQAY86Hdz4Ae9fAn75CcW42337vRWzY3cRtP1kdua6+qR5e/iE8cjN8exo88Qk48KobCOz2x2Huh5P5KZzBJiBpa3Ft4WfcDOMuHP55Fn4RAofg1V8M/xjGmKSLZqybzDX7XW7Qs5fvh6p5vPOS91OY4+WTj77C+3/8Mr/4+wWMLc5xwx6/8Qf3OLzJ7TtmlmuHfv7NMO5iV7JOlZJq2PmnyOvXPuh+bVwzzNJ8yKS3wPjL4cXvwaV3gNcX2/GMMUkxshM9wPX/BAc2wv9+Bipmc/2sOfzszkv58S9+xfM/eJB352/Ee2IfIDDhcrjxn11zw/KpqY78NP8ECBx2VSrZuWeuaz/hLmTTb4LKS2I7j4i7WPzyPfDar2FuYqcXMMbEhyV6jxfe+wj8eKHrTDXhCi7f8RSXSxPt3dn8NXAh0675DJWXvQsKR6c62v6Fmli27D/7ArTuv1xnqmu+FJ9zTb0OKi91c8xe/EH3/Rlj0trIraMPVzgG3vszlyi3r3Sl3/f9nH0f3czdvq+w+C8T2dCQxLbxQ9Xblr7PDdmOk264g6nXQfW8+JxLxNXVN9XDpt/G55jGmISyRB8yYQF8bit8sRbe/WOYtYSpVWP57cevoKzAx4ceWsvzO46mOsr+RZqAZP0jcOqYm4glnma8DSougBe+DT3d8T22MSbuLNGHKxwNnuwzFlWX5vPbj1/JpFEF/P3P1rFyUwzzsyZKcRUgZyb6zlY3Q9Tka9xFLJ5E3MiWDbWw9fH4HtsYE3eW6KMwuiiH5csu56JqP3c9+gqPrduT6pDO5PVB0dgze8du+BmcPBL/0nzIzFtg1Ax4/tvQE4e5do0xCWOJPkoledn8/O/n85bpo7n7vzfxk+d3pTqkM4V3mupsc+PvT7waJl2VmPNlZblS/ZGtsP0PiTmHMSYuLNEPQb7Py0/+dh43XziOb6zcxr89/UZso13GU3iif/UXcOIgLEpQaT5k9rvdUBJ/+dehjYefiXq6Yfsf8Tdtsl84Ju1Y27gh8nmz+MHSSyjO9fLAqjdpbu3kvlvmkJWVwg5T4G7IvrHSleZf/C5MuMJ1cEokj9d1Gnvik67D1nk3JfZ86ai7E17/jWtu2lDLxQB1/+nG+7/wfVAxJ7Wd6YzBEv2weLKE//euCyjOy+bHf9nFibYuvv3ei8j2pPAHUsl4N2vUS993zUSX3J+cBHPh++Ev33Kl+uk3Du+cXe1wdDsc2eaGeC6udPcciirP7gCWLjrbYOMv4cXvQ/MeGHsBvPenbN28iVldm934/X/9AYye6RL+Be910z4akwJRJXoRWQx8H/AAD6nqNyNs9x7gd8BlqrpeRCbhph/cHtxktap+POao04CI8OW3zaQkL5t//eN2TrR18cBtl5LnS1F7+1ASeeHfofoymHJtcs7ryYarPwdPfg521cDUQc4bOOKGlDi8GQ5tds/Hdri5dvuTV+oSfvE4KBrHpMYOKNwVtqzSjeCZlaSLbMdJ12z1r//hxv2pvgxu/nbvRe7I0VJmLbrXDXi35X9cX4M//5N7TLjSJf1ZSyC/LDnxGkMUiV5EPMADwA3APmCdiKxQ1a19tisCPgOs6XOIN1X14viEm34+sWgaJXnZ/MPjm7nj4bU8dOc8inOzB98x3kKdprrbXUubZFYXXPxB+Mu/wfP/djrRd3e6BH5osxsfKJTUT4b1RSiqdDNYnbfYPY+ZDagbOvrEIThxAFoOuvsNLQfg0GYmBg7D7sfOPH9WNhSNc78Cxl4Ak9/ibkTHsydzW7MbM+jlH0JroxvJ890Puuf+vuuCcpj/UfdorINNv4NNv4EnPwsrv+guDBe+13327Lz4xWlMP6Ip0c8HalV1F4CILAeWAFv7bPd14FtAjCNnnXs+uGAixbnZfO6xjSy5/yV+9KG5zBhblNwgQom+8hI3SUgyeXPchC5/vNsNI9FYD0ffgJ5Ot97jg9Hnu+RWMccl9Yo5kUu1Y2ZGPNXzz/2Za+aefzr5h18QWva7+vL1/+U2Hj0TJl19OvEXlA/9s51scNUwa38C7c2u1/TCL8D4+dEfo2yyGyNo4Rfg4GuulL/pd661Uk6xa6p64XvdPZVkzk5mRgwZrNWIiNwKLFbVjwTf3w4sUNW7wra5FPiKqr5HRGqAL4RV3WwBdgAtwD+o6gv9nGMZsAygoqJi7vLly4f9gQKBAIWFhcPePxbbG7v54WvttHYpH56dw+WVZ19HExafKpPqH+XYqAUEiqYN+zDDjS+ru5156z+Dp7uNkwWTCBROJlDonlvzKtGs+NwOGiw+6emmMFCL//hmSps2UdK8DU+PGz8/UDCR4/45HPdfwHH/bLqyiyMex9feyPi9T1B54Cmyejo4Nupydk98H4GiKTHF10u7KW3axJgjf2H00ZfxdrfS7iujpXgGndnFdPhK6MwuDj5KzlimWcP/xZjK/x/RiObf19t1guzOQPDZPbxdAbI7W+jy5nOyYDKBwsl05JQmJT5v5wkKA3UUnNyNipfGsktpy6uI+7kHc+21125Q1X7HOok50YtIFvAccKeq1vdJ9DlAoao2iMhc4HFgtqpGnGV63rx5un79+iF/yJCamhoWLVo07P1jdaSljU8++grr6pu488pJ/N+3z8TnPV1/nOr4BhNTfKoJrzIacnzdnW6+gPoXoO4FN/9A5ylA3K+K3hL/le5+wPE97ob2K79wv0gueC9c/Xk3f0Ei4gPXi3nHH10pv6EWTh5z1UMaoZlmTrG7L1EwCvJHuV8q+aPce1+h+wXlyQ4+fGHvfWx4bRNzL7vcVXeFLT/jtTcncf+O3V3QcQLaA25ay/bAGe93bt7A9KpyN79xa5P7HnpfN7nJcyKRrDO/s4LRriqvYg6MvdD9kiyfPvyB+Hq6WfPHX7NgQv6Z95ha9p+9bfl0mH6D+3U98aqkNCoQkYiJPppPvB8Iby5QHVwWUgTMAWrE/XGMBVaIyC2quh5oB1DVDSLyJnAeMPxMnubGFOfy6Ecv519WvsHDL9WxeX8zD3zwUiqK07T1SDylYzNCT7arZhk/3zUF7eqAA6+cTvwbHoE1/wmIq15q2OleX3wbXP1Z108g0bLz3NwIs991ellPt5v+8dQxl/h7nxvOfN+8113ITjWcriobwFyAV6KIyeNzU1SGnr3h73Pcw5Nz+nVoXU9XWAI/O5HTNfDsZNMBagXy/O7Cm1fmEvboGaff55W6ar88/5nvfUWueu3wFnfD/9BmOPS6m0KzOziRkDfXVQ2GJ/+K2ZBbcmYgbS3uOIc3n248cGQbCzpPwVpAPC6miVedrooce4G7l7PzGah9xo0cu/qHkJ3vquWmXQ/Tr0/O31Qf0ST6dcB0EZmMS/BLgdtCK1W1GRgVet+nRD8aaFTVbhGZgvt3TLMupfGX7cni3r+ZxSUT/Nz9369z8w9e5IHbLmHBlGHUEZv48vrcvAITLnejcHa1w/4NbqL4PS/DlGvgyk+dvueRKlkeV1IvKHcJZTCqLsl0trqk1t0ZfO5wyTf4+vVXN3Dh7POD6/ts09XuXne1u5v6XWGP7naXpLs63HN3h5vroKsjbNs2yPK6XxU5he7ZP/7M9zlFA75/ccMmrr7u5uHfq8grdb/SJl19ellvw4BNpx/bV545U5p/okvU4NYf333mMSvmwNw7eaPJy/mLbnWFAm/O2ecvHAOjpsMVn3BTlda/6JL+zmdg59PwFFA2NVjav8H1XE/CzfhBE72qdonIXcDTuOaVD6vqFhG5D1ivqisG2H0hcJ+IdAI9wMdVtTEegZ8L/uaiSmaMLeLjv9zAbQ+t4Z7F5zNtpPcgTTfeHFdtM/HKVEcSGwmVgv0Dbta4Bzh/URICGp6u7N3xvyHtyXal9orZcNFSt0zV3dAPlfpDJXdwDRouvd2V+CvmuH4dwV+rh2pqOH/cRdGd15cP593oHgANb0Ltsy7pb/ip+6XhzXUXpWk3uOSfoAmNoqqsUtWVwMo+y+6NsO2isNf/Dfx3DPGd886rKOKJT17FF3/7Ot9YuY15FR4uu7KLwhzrq2ZMyoi4BF5ceToRJ1r5VPdY8DH3y6v+JZf4a59xLdb+eLerCvq7lYMfa4gs2yRBUW42//mhS3nw+V1886k3WHL/i/z49rlMG5PkJpjGmPSQnefq66dfD3zT9bWofTZhY0bZoGZJIiJ87JqpfOmyXJpbO1ly/0v84fU0HNveGJN8ZZNd57oFyxJyeEv0STaz3MOTn3oLM8YW8clHX+Ebf9hKV7eNdmiMSRxL9CkwtiSX5cuu4I4rJvKTF+q47aE1HDkxcLMzY4wZLkv0KeLzZvFPS+bwvfdfzOv7jvOOH7zI+voR0yDJGJNEluhT7J2XVPH4J68i3+dh6YOr+fxjG1m56SCB9gijORpjzBBZq5s0cP7YYlZ86mr+ZeUbPLX5IP/z6n58niwun1rODTPHcN3MCir9NsKhMWZ4LNGnieLcbP7l3Rfw9SWz2bC7iWe3HebZbUf4xye28I9PbGF2ZTHXzazghpkVzKkqRtJxuAFjTFqyRJ9mvJ4sFkwpZ8GUcr5y8yzePBrg2a2HeXbbYe5/bic/+PNOxhbnct3MMVw/q4IrppSTm21D2xpjIrNEn+amji5k6jWFfOyaqTQE2lm1/SjPbj3M71/dz6/W7CHf52Hh9NFcP6uCa2eMprywn/E3jDEjmiX6c0h5YQ63zq3m1rnVtHV2s3pXg6vi2XqEP245RJbATbPH8pnrp3P+2MhjrRtjRhZL9Oeo3GwPi2aMYdGMMXx9ibLlQAt/2HSQX7y8m6c2H+LmC8bxmeunc16FDbNgzEhniT4DiAhzqkqYU1XCxxZO4aEX6njkpTpWbj7oEv5105luCd+YEcva0WcYf76PL9w0gxfvfiufWDSVVW8c4cbvPc+nfv0qtUdOpDo8Y0wKWKLPUKUFPr540/m8cPdb+fg1U/nztsPc8N3n+czyV6k9Ekh1eMaYJLJEn+HKCnzcvfh8XvjStSxbOIU/bTnMjd/9C59d/iq7jsYn4bd2dLPlQDNPbTrI/uOtcTmmMSZ+rI5+hCgvzOHLb5vJR98yhQef38XPX65nxWsHeOfFVXzquulMHlUw4P6qyuGWdt48GuDNowF2HT3Z+xye3H3eLD581WQ+ce1UinOzE/2xjDFRsEQ/wowqzOH/vj2U8N/kF6t380Qo4b91Gh3dyraDLb2JPJTMdx0NcLKju/c4BT4PU0YXctmkUt4/ejxTRxcytiSXX63ezY/+8ia/Wb+Xz14/nQ/Mn0C2x344GpNKUSV6EVkMfB83Z+xDqvrNCNu9B/gdcJmqrg8u+zLw90A38GlVfToegZvYjC7K4Ss3z+KjC6fw47/s4perd/P7V/ehCvrMC73bVfnzmDK6gPfOG8/U0QVMHV3IlNGFVBTn9DsMw9yJpXz46sn88x+2cu8TW/jpS/Xc87bzuWFWhQ3bYEyKDJroRcQDPADcAOwD1onIClXd2me7IuAzwJqwZbOApcBsoBJ4VkTOU9VuTFoYU5TLP75jFh9bOIVfrtnDnt31vPWyOUwdXcDkUQXk+4b+o29OVQm//ujl/HnbEf7fU9tY9osNLJhcxldunsmF1f74fwhjzICi+U09H6hV1V2q2gEsB5b0s93XgW8B4TNoLAGWq2q7qtYBtcHjmTQzpjiXz99wHu+c5uOWiyqZXVkyrCQfIiJcP6uCpz+7kK+/cw61RwLccv9LfO6xjXbD1pgkEx1kMloRuRVYrKofCb6/HVigqneFbXMp8BVVfY+I1ABfUNX1InI/sFpVfxnc7r+Ap1T1d33OsQxYBlBRUTF3+fLlw/5AgUCAwsLCYe+faCM1vlOdysq6Tp6u70SBmyZm846p2eR5h1adM1K/v3ix+GKTzvFde+21G1R1Xn/rYr4ZKyJZwHeAO4d7DFV9EHgQYN68ebpo0aJhx1NTU0Ms+yfaSI7v7cD+4618++nt/P7V/aw+Inz2+uksHcIN25H8/cWDxRebdI8vkmj+d+0Hxoe9rw4uCykC5gA1IlIPXA6sEJF5UexrRpgqfx7fff/F/O9dVzNtTCH/+MQWbvre8zyz9TCD/bo0xgxPNIl+HTBdRCaLiA93c3VFaKWqNqvqKFWdpKqTgNXALcFWNyuApSKSIyKTgenA2rh/CnPOuaC6hOXLLucnf+t+aX705+v5wE9Ws+K1A9QdO0lPjyV9Y+Jl0KobVe0SkbuAp3HNKx9W1S0ich+wXlVXDLDvFhH5DbAV6AI+aS1uTIiIcMOsChbNGM3ytXv43rM7+fSvXwWgKNfLBVUl7lHtnieU5ac4YmPOTVHV0avqSmBln2X3Rth2UZ/33wC+Mcz4zAiQ7cni9ismsXT+BHYcPsGmfc28vr+ZzfubeeSlejq6ewAoycumOr+b1a1vcGEw+VeX5ln7fGMGYT1jTdrI9mQxu7KE2ZUlLA0u6+jqYcfhE7y+r5lN+5v56xv7+K8Xd9HZ7ap2/PnZvSX/C6tLmDyqkLHFuRTneRN6AejpUQ61tLGn8RR7Gk6558ZTHG9op9azi6ljCpk2upAqfx5ZWXYhMqllid6kNZ83q3esfYCamgauuPotbD/kkv/m/c28vq+ZB5/fRVdYvX5udhbjSvKoKM5hbHEuFSW5jCvOZWxJLhXFuYwryWNUoQ/vAK19Wju62dt0it2hRN5wkj2Np9jdeIp9ja29vzQAPFlCpT+XphNdPP+Hbb3L87I9TBldwLRg4p86ppBpYwqZVF6Az2tDQ5jksERvzjk5Xg8XVvvP6GXb1tnNG4dOsLfxFIdb2jjU3Mah4PP63U0caWk/IzEDZIkbCmJsSR5ji3OoKM4l0N7VW0I/cqL9jO2LcrxMKM9nRkURN8yqYEJZPhPLCphQlk+lPxevJ4uamhouvOxKao8ETj+OBlhf38QTGw/0HsuTJUwsy+9N/NNGu+cJZfn487OtOsrElSV6kxFysz1cPN7PxeP9/a7v6VGaTnVwsLnNXQhCF4PgBaHu2ElefrOBwhwv48vyWTRjNBPK8plQXhBM6NEn4LICH/MnlzF/ctkZy0+2d1F37ORZF4FVbxw549dIjjeLsSW5jA3+Ahkb9mvEXZRyGV2Ug8eqhEyULNGbESErSygvzKG8MKe3GijZCnK8Z1RDhXR297C74RS1RwLsP97K4ZY2d0FqbuOVPU0cbj7714gnSxhTlBOshsrtvTCcN7aIuRNLbYhocwZL9MakWLYny1XfjOm/a72q0njy9K+R8OdDzW3sPBLghZ3HCLR3Aa5Kaua4YverYlIZl00uY1RhTjI/kkkzluiNSXMi0f0aaWnrZPO+ZtbWN7K2rpFfr93DIy/VAzBldAELJpdR1NbFtKZTVJdan4SRxBK9MRmiODebK6eN4sppowDXNHXzgWbW1jWyrq6RJ18/yIm2Lh58fRVV/jzmTy7jsknuXsLU0QVDugHc06O0dXVzqqOb1o7gc2c35QU+69uQhizRG5OhfN4sLp1QyqUTSvn4NVPp7lF+9eRz6KiprK1r5MXaY/z+VTf0VHmBj8smlTGmOCcseXdxqqObtk6XyEPJ/FRHF22dPRHPW17g46Lxfi6sLuGi8X4uqvZTVuBL1sc2/bBEb8wI4ckSJhR7WHTlJO64chKqSn3DKdbWNbC2rol19Y2sqesk3+clz+chL9tDns9DaYGPSr97ne/zkO/zkpsden16u7xsDweb23ht73Fe23ecVduPEBqnbkJZfjDpl3DxeD+zK0vI83lS+4WMIJbojRmhRITJo9xMYu+/bELcjvuhyycCEGjvYtO+Zl7bd5zX9h7nld1N/O9rri+BJ0uYUVHEReP9XDzelfynjymKWwzJoKrsa2pl8/5mthxoYfOBZrxZwvzJZSyYXM7syuIBO+QlkyV6Y0xCFOZ4uWJqOVdMLe9dduREG6/vdcl/497j/OH1A/x67R4A8n0eSn09jNn6EoU5Xgp8XgpyvBTmeCjICb0eYJnPS2GuNyH9C7p71PW1ONDFS3/Yyub9LWw50ExLm2vp5MkSpo8ppKOrh2e3Hen9/PMmlbJgcjmXTyljTlVJ1PMuxJslemNM0owpyuX6WblcP6sCoLf6KFTd83rtXvJzvATauzjc0sbJ9m4C7V2cbO86o1NZJFkCpfk+ygt9lBfkUFboY1SBj/LCHMoKfIwqDHtdkNPvmEid3T3sPBxg84FmtuxvZvOBFrYeaKG10w286/PuZubYIt5xUSVzKkuYU1XMeRVF5Ga7qqgjLW2srmtkza4G1tQ1UrP9DcBdyOZOLOXyKS7xX1DlT9owGJbojTEpE1599M5LqqipOcqiRQvO2k5Vae/q4WR71+nk39HVexE42d7FibYuWlo7OXayg4ZAO40nO9h2oIVjgfbekndf3iyhLHghKC/w0dzayfZDJ3o7qBX4PMyqLOb9l41nTlUJrQd2sPTtiwYsmY8pzuWWiyq55aJKAI6eaGdtXSNr6hpYvauBf3t6O+DGY5o7MVTiL+ei8SXkeBNz38ISvTEm7YkIudkecrM9lA9jytaOrh6aTnVwLHgBaAic+brhZDvHAh0U53n5u6smMbuqhDmVxUwqLzhj9NGaE7VDrn4ZXZTDzReO4+YLxwHQEGhnXX0jq3c1snpXA995Zgfghr64YVYF99926dA/4CAs0RtjMp7Pm0VFsRu5NNXKC3NYPGcci+e4xH/8VAdr61ziz/MlpirHEr0xxqSQP9/HjbPHcuPssQk7R1SXDxFZLCLbRaRWRO7pZ/3HRWSTiGwUkRdFZFZw+SQRaQ0u3ygiP4r3BzDGGDOwQUv0IuIBHgBuAPYB60RkhapuDdvsUVX9UXD7W4DvAIuD695U1YvjGrUxxpioRVOinw/UquouVe0AlgNLwjdQ1ZawtwXA4O2gjDHGJIWoDpyTReRWYLGqfiT4/nZggare1We7TwKfB3zAW1V1p4hMArYAO4AW4B9U9YV+zrEMWAZQUVExd/ny5cP+QIFAgMLCYdyWTxKLLzYWX2wsvtikc3zXXnvtBlWd1+9KVR3wAdwKPBT2/nbg/gG2vw34WfB1DlAefD0X2AsUD3S+uXPnaixWrVoV0/6JZvHFxuKLjcUXm3SOD1ivEfJqNFU3+4HxYe+rg8siWQ68M3gRaVfVhuDrDcCbwHlRnNMYY0ycRJPo1wHTRWSyiPiApcCK8A1EZHrY25uBncHlo4M3cxGRKcB0YFc8AjfGGBOdQVvdqGqXiNwFPA14gIdVdYuI3If7qbACuEtErgc6gSbgjuDuC4H7RKQT6AE+rqqNifggxhhj+jfozdhkE5GjwO4YDjEKOBancBLB4ouNxRcbiy826RzfRFUd3d+KtEv0sRKR9RrpznMasPhiY/HFxuKLTbrHF0l6jIpvjDEmYSzRG2NMhsvERP9gqgMYhMUXG4svNhZfbNI9vn5lXB29McaYM2Viid4YY0yYczLRRzFsco6IPBZcvyY45k6yYhsvIqtEZKuIbBGRz/SzzSIRaQ4bvvneZMUXFkN92NDS6/tZLyLyg+B3+LqIxH/am8ixzQj7bjaKSIuIfLbPNkn9DkXkYRE5IiKbw5aVicgzIrIz+FwaYd87gtvsFJE7+tsmQfH9m4i8Efz3+72I+CPsO+DfQgLj+5qI7A/7N3x7hH0H/P+ewPgeC4utXkQ2Rtg34d9fzCKNjZCuD1ynrTeBKbgB1F4DZvXZ5hPAj4KvlwKPJTG+ccClwddFuAHd+sa3CHgyxd9jPTBqgPVvB54CBLgcWJPCf+9DuDbCKfsOcZ3/LgU2hy37V+Ce4Ot7gG/1s18Zrjd4GVAafF2apPhuBLzB19/qL75o/hYSGN/XgC9E8e8/4P/3RMXXZ/2/A/em6vuL9XEulugHHTY5+P5nwde/A64T6TPVe4Ko6kFVfSX4+gSwDahKxrnjbAnwc3VWA34RGZeCOK7DzWkQSye6mKnq80DfXt3hf2c/IzjGUx83Ac+oaqOqNgHPcHquhoTGp6p/UtXQrNirceNUpUSE7y8a0fx/j9lA8QVzx/uAX8f7vMlyLib6KtwomCH7ODuR9m4T/ENvBsqTEl2YYJXRJcCaflZfISKvichTIjI7uZEBbs6AP4nIhuAw0X1F8z0nw1Ii/wdL9XdYoaoHg68PARX9bJMu3+OHcb/Q+jPY30Ii3RWsWno4QtVXOnx/bwEOq+rOCOtT+f1F5VxM9OcEESkE/hv4rJ45MQvAK7iqiIuA/wAeT3J4AFer6qXA24BPisjCFMQwIHGD6N0C/Laf1enwHfZS9xs+LZuwichXgC7gVxE2SdXfwn8CU4GLgYO46pF09AEGLs2n/f+lczHRRzNscu82IuIFSoCGpETnzpmNS/K/UtX/6bteVVtUNRB8vRLIFpFRyYoveN79wecjwO9xP5HDDXV46kR4G/CKqh7uuyIdvkPgcKg6K/h8pJ9tUvo9isidwDuADwYvRmeJ4m8hIVT1sKp2q2oP8JMI50319+cF3g08FmmbVH1/Q3EuJvpBh00Ovg+1brgVeC7SH3m8Bevz/gvYpqrfibDN2NA9AxGZj/t3SOaFqEBEikKvcTftNvfZbAXwt8HWN5cDzWHVFMkSsSSV6u8wKPzv7A7giX62eRq4UURKg1UTNwaXJZyILAa+BNyiqqcibBPN30Ki4gu/5/OuCOeN5v97Il0PvKGq+/pbmcrvb0hSfTd4OA9ci5AduLvxXwkuuw/3Bw2Qi/u5XwusBaYkMbarcT/hXwc2Bh9vBz6OG6YZ4C7cFIuv4W6SXZnk729K8NyvBeMIfYfhMQpuUvg3gU3AvCTHWIBL3CVhy1L2HeIuOAdxQ3HvA/4ed9/nz7j5F54FyoLbzuPMWdk+HPxbrAX+Lonx1eLqt0N/h6GWaJXAyoH+FpIU3y+Cf1uv45L3uL7xBd+f9f89GfEFl/809DcXtm3Sv79YH9Yz1hhjMty5WHVjjDFmCCzRG2NMhrNEb4wxGc4SvTHGZDhL9MYYk+Es0RtjTIazRG+MMRnOEr0xxmS4/w+vq7C7V3xSGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.4063\n",
      "[[0.43741655]\n",
      " [1.690215  ]\n",
      " [4.2978745 ]]\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate((X_test_A,X_test_B),y_test)\n",
    "y_pred = model.predict((X_new_A,X_new_B))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Output\n",
    "\n",
    "There are also many use cases in which you may want to have multiple output. \n",
    "- If you want to **locate** and **classify** the main object in a picture. This is both a regression task and classification task.\n",
    "\n",
    "- When you have multiple independent tasks to perform based on the same data. \n",
    "\n",
    "- As a regularization technique (i.e training constraint whose objective is to reduce overfitting and thus improve the model's ability to generalize)\n",
    "\n",
    "\n",
    "For adding extra output, connect them to the appropriate layers and add them to your model's list of outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape = [5],name = \"wide_input\")\n",
    "input_B = keras.layers.Input(shape = [6],name = \"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30,activation = 'relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30,activation = 'relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A,hidden2])\n",
    "output = keras.layers.Dense(1,name = \"Main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1,name = \"Aux_output\")(hidden2)\n",
    "\n",
    "model = keras.models.Model(inputs = [input_A,input_B],outputs = [output,aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "deep_input (InputLayer)         [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 30)           210         deep_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "wide_input (InputLayer)         [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 30)           930         dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 35)           0           wide_input[0][0]                 \n",
      "                                                                 dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Main_output (Dense)             (None, 1)            36          concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Aux_output (Dense)              (None, 1)            31          dense_24[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,207\n",
      "Trainable params: 1,207\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we pass single \"mse\", mse will be used for both output\n",
    "model.compile(loss = [\"mse\",\"mse\"], loss_weights = [0.9,0.1],optimizer = \"sgd\")\n",
    "# Keras will compute losses and add simply add them to get final loss. \n",
    "# if we care much more about main output then aux output then, we assign\n",
    "# weights accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8447 - Main_output_loss: 0.7632 - Aux_output_loss: 1.5784 - val_loss: 0.5905 - val_Main_output_loss: 0.5413 - val_Aux_output_loss: 1.0334\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5296 - Main_output_loss: 0.4920 - Aux_output_loss: 0.8681 - val_loss: 0.5445 - val_Main_output_loss: 0.5135 - val_Aux_output_loss: 0.8235\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4824 - Main_output_loss: 0.4529 - Aux_output_loss: 0.7478 - val_loss: 0.4955 - val_Main_output_loss: 0.4679 - val_Aux_output_loss: 0.7440\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4415 - Main_output_loss: 0.4161 - Aux_output_loss: 0.6700 - val_loss: 0.4931 - val_Main_output_loss: 0.4694 - val_Aux_output_loss: 0.7062\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4298 - Main_output_loss: 0.4075 - Aux_output_loss: 0.6301 - val_loss: 0.4819 - val_Main_output_loss: 0.4598 - val_Aux_output_loss: 0.6805\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4205 - Main_output_loss: 0.3999 - Aux_output_loss: 0.6060 - val_loss: 0.4975 - val_Main_output_loss: 0.4774 - val_Aux_output_loss: 0.6786\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4187 - Main_output_loss: 0.4001 - Aux_output_loss: 0.5859 - val_loss: 0.4827 - val_Main_output_loss: 0.4620 - val_Aux_output_loss: 0.6691\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4126 - Main_output_loss: 0.3946 - Aux_output_loss: 0.5742 - val_loss: 0.4911 - val_Main_output_loss: 0.4707 - val_Aux_output_loss: 0.6746\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4063 - Main_output_loss: 0.3889 - Aux_output_loss: 0.5629 - val_loss: 0.4807 - val_Main_output_loss: 0.4606 - val_Aux_output_loss: 0.6616\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4040 - Main_output_loss: 0.3872 - Aux_output_loss: 0.5557 - val_loss: 0.4753 - val_Main_output_loss: 0.4563 - val_Aux_output_loss: 0.6463\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4015 - Main_output_loss: 0.3855 - Aux_output_loss: 0.5454 - val_loss: 0.4804 - val_Main_output_loss: 0.4614 - val_Aux_output_loss: 0.6519\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3971 - Main_output_loss: 0.3813 - Aux_output_loss: 0.5394 - val_loss: 0.4751 - val_Main_output_loss: 0.4559 - val_Aux_output_loss: 0.6481\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3937 - Main_output_loss: 0.3783 - Aux_output_loss: 0.5315 - val_loss: 0.4755 - val_Main_output_loss: 0.4570 - val_Aux_output_loss: 0.6420\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3894 - Main_output_loss: 0.3742 - Aux_output_loss: 0.5261 - val_loss: 0.4743 - val_Main_output_loss: 0.4561 - val_Aux_output_loss: 0.6373\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3877 - Main_output_loss: 0.3731 - Aux_output_loss: 0.5188 - val_loss: 0.4739 - val_Main_output_loss: 0.4560 - val_Aux_output_loss: 0.6346\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3845 - Main_output_loss: 0.3699 - Aux_output_loss: 0.5157 - val_loss: 0.4748 - val_Main_output_loss: 0.4571 - val_Aux_output_loss: 0.6335\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3822 - Main_output_loss: 0.3680 - Aux_output_loss: 0.5102 - val_loss: 0.4770 - val_Main_output_loss: 0.4595 - val_Aux_output_loss: 0.6352\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3797 - Main_output_loss: 0.3658 - Aux_output_loss: 0.5048 - val_loss: 0.4722 - val_Main_output_loss: 0.4546 - val_Aux_output_loss: 0.6304\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3791 - Main_output_loss: 0.3655 - Aux_output_loss: 0.5007 - val_loss: 0.4741 - val_Main_output_loss: 0.4574 - val_Aux_output_loss: 0.6247\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3778 - Main_output_loss: 0.3646 - Aux_output_loss: 0.4962 - val_loss: 0.4685 - val_Main_output_loss: 0.4515 - val_Aux_output_loss: 0.6212\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train_A,X_train_B],[y_train,y_train],epochs = 20,\n",
    "    validation_data = ([X_valid_A,X_valid_B],[y_valid,y_valid])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 777us/step - loss: 0.4244 - Main_output_loss: 0.4076 - Aux_output_loss: 0.5758\n"
     ]
    }
   ],
   "source": [
    "total_loss,main_loss,aux_loss = model.evaluate([X_test_A,X_test_B],[y_test,y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40762612223625183\n",
      "0.5757866501808167\n",
      "0.4244421720504761\n"
     ]
    }
   ],
   "source": [
    "print(main_loss)\n",
    "print(aux_loss)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.53341085]\n",
      " [1.949464  ]\n",
      " [4.278716  ]]\n",
      "[[0.62737286]\n",
      " [2.263744  ]\n",
      " [4.2259927 ]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_main,y_pred_aux = model.predict([X_new_A,X_new_B])\n",
    "print(y_pred_main)\n",
    "print(y_pred_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Subclassing API (For dynamic models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self,units = 30,activation = \"relu\",**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units,activation = activation)\n",
    "        self.hidden2 = keras.layers.Dense(units,activation = activation)\n",
    "        self.main_output = keras.layers.Dense(1,name = \"main_output\")\n",
    "        self.aux_output = keras.layers.Dense(1,name = \"aux_output\")\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        input_A,input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A,hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        \n",
    "        return main_output,aux_output\n",
    "    \n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.2231 - output_1_loss: 2.0005 - output_2_loss: 4.2267 - val_loss: 1.0602 - val_output_1_loss: 0.8303 - val_output_2_loss: 3.1295\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.9109 - output_1_loss: 0.7164 - output_2_loss: 2.6613 - val_loss: 0.8413 - val_output_1_loss: 0.6955 - val_output_2_loss: 2.1541\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7703 - output_1_loss: 0.6407 - output_2_loss: 1.9365 - val_loss: 0.7551 - val_output_1_loss: 0.6504 - val_output_2_loss: 1.6978\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7006 - output_1_loss: 0.6039 - output_2_loss: 1.5708 - val_loss: 0.7090 - val_output_1_loss: 0.6262 - val_output_2_loss: 1.4538\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6555 - output_1_loss: 0.5761 - output_2_loss: 1.3703 - val_loss: 0.6716 - val_output_1_loss: 0.5989 - val_output_2_loss: 1.3259\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6220 - output_1_loss: 0.5518 - output_2_loss: 1.2539 - val_loss: 0.6470 - val_output_1_loss: 0.5808 - val_output_2_loss: 1.2427\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5959 - output_1_loss: 0.5314 - output_2_loss: 1.1761 - val_loss: 0.6242 - val_output_1_loss: 0.5620 - val_output_2_loss: 1.1846\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5745 - output_1_loss: 0.5141 - output_2_loss: 1.1181 - val_loss: 0.6064 - val_output_1_loss: 0.5470 - val_output_2_loss: 1.1411\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5563 - output_1_loss: 0.4988 - output_2_loss: 1.0734 - val_loss: 0.5914 - val_output_1_loss: 0.5344 - val_output_2_loss: 1.1044\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5410 - output_1_loss: 0.4861 - output_2_loss: 1.0358 - val_loss: 0.5771 - val_output_1_loss: 0.5221 - val_output_2_loss: 1.0722\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.5224 - output_1_loss: 0.4681 - output_2_loss: 1.0110\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fae9fac1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,\n",
    "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example looks very much like Functional API, except we do not need to create the inputs, we just use the input argument to call() method, and we separate the creation of layers in the constructor from their usage in the call() method.\n",
    "\n",
    "you can do anything you like with call statements. for loops, while, if statements, etc. This is great for experimenting with new ideas. \n",
    "\n",
    "\n",
    "However, your model's architecture is hidden within the call() function, so keras cannot easily inspect it, it cannot save or clone it, and when you call summary() method, you only get a list of layers, without any information on how they are connected to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"wide_and_deep_model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             multiple                  210       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             multiple                  930       \n",
      "_________________________________________________________________\n",
      "main_output (Dense)          multiple                  36        \n",
      "_________________________________________________________________\n",
      "aux_output (Dense)           multiple                  31        \n",
      "=================================================================\n",
      "Total params: 1,207\n",
      "Trainable params: 1,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.8866 - val_loss: 0.7414\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6577 - val_loss: 0.6221\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5934 - val_loss: 0.5779\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5557 - val_loss: 0.5572\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 0.5305\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5033 - val_loss: 0.5113\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4854 - val_loss: 0.4974\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4709 - val_loss: 0.4903\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4578 - val_loss: 0.4773\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4474 - val_loss: 0.4696\n",
      "162/162 [==============================] - 0s 673us/step - loss: 50316.0156\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10, validation_data=(X_valid_scaled, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving weights.\n",
    "model.save_weights(\"my_keras_weights.ckpt\")\n",
    "# saving model.\n",
    "model.save(\"my_keras_mode.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will typically have a script that trains a model and saves it. But what if the training lasts for hours? This is quite common, especially when training on large datasets. In this case, you should not only save your model at the end of training, but also save checkpoints at regular intervals during training. You can do this by using **Callbacks**.\n",
    "\n",
    "## Using Callbacks during Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4393 - val_loss: 0.4655\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4315 - val_loss: 0.4627\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.4562\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.4554\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4154 - val_loss: 0.4506\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.4461\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4074 - val_loss: 0.4446\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4040 - val_loss: 0.4434\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4008 - val_loss: 0.4391\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3976 - val_loss: 0.4363\n",
      "162/162 [==============================] - 0s 722us/step - loss: 24681.2285\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",save_best_only = True)\n",
    "# save best only true means it will only save your model when its performance on validation\n",
    "# set is the best so far. \n",
    "history = model.fit(X_train_scaled, y_train, epochs=10,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to implement early stopping is to use EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by patience argument), and it will optionally roll back to the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3949 - val_loss: 0.4344\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3923 - val_loss: 0.4354\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3898 - val_loss: 0.4317\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3873 - val_loss: 0.4311\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3851 - val_loss: 0.4287\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3829 - val_loss: 0.4266\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.4262\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3788 - val_loss: 0.4252\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3768 - val_loss: 0.4231\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3749 - val_loss: 0.4218\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3733 - val_loss: 0.4208\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3716 - val_loss: 0.4204\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3700 - val_loss: 0.4197\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.4192\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3671 - val_loss: 0.4181\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3659 - val_loss: 0.4156\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3648 - val_loss: 0.4157\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.4160\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.4153\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3613 - val_loss: 0.4145\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.4140\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3590 - val_loss: 0.4155\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3583 - val_loss: 0.4133\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3571 - val_loss: 0.4140\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.4116\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3553 - val_loss: 0.4118\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.4107\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3538 - val_loss: 0.4113\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3532 - val_loss: 0.4103\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3523 - val_loss: 0.4106\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.4100\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 0.4119\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.4083\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.4091\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.4098\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.4089\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.4088\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.4088\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.4080\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.4074\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3454 - val_loss: 0.4066\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.4080\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3442 - val_loss: 0.4080\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3439 - val_loss: 0.4071\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.4078\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3426 - val_loss: 0.4074\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3425 - val_loss: 0.4075\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3419 - val_loss: 0.4057\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3416 - val_loss: 0.4051\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3410 - val_loss: 0.4072\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3405 - val_loss: 0.4058\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3401 - val_loss: 0.4079\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3397 - val_loss: 0.4082\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3393 - val_loss: 0.4040\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3390 - val_loss: 0.4067\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3385 - val_loss: 0.4056\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3378 - val_loss: 0.4060\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3375 - val_loss: 0.4086\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3374 - val_loss: 0.4055\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3367 - val_loss: 0.4062\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3362 - val_loss: 0.4047\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3361 - val_loss: 0.4044\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3352 - val_loss: 0.4070\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3352 - val_loss: 0.4041\n",
      "162/162 [==============================] - 0s 656us/step - loss: 11025.4502\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[checkpoint_cb,early_stopping_cb])\n",
    "\n",
    "# The number of epochs can be set to a large value since training will stop automatically when there is no more progress. \n",
    "mse_test = model.evaluate(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need extra control, you can write your custom callbacks.\n",
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"]/logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343/363 [===========================>..] - ETA: 0s - loss: 0.3358\n",
      "val/train: 1.20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3384 - val_loss: 0.4052\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train_scaled,y_train,epochs = 1,\n",
    "                   validation_data = (X_valid_scaled,y_valid),\n",
    "                   callbacks = [val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization using Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "root_logdir = os.path.join(os.curdir,\"my_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./my_logs/run_2021_05_20 - 09_24_13\n"
     ]
    }
   ],
   "source": [
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d - %H_%M_%S\")\n",
    "    return os.path.join(root_logdir,run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "print(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/363 [..............................] - ETA: 0s - loss: 0.6787WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_train_batch_end` time: 0.0101s). Check your callbacks.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.4204\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3714 - val_loss: 0.4214\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3700 - val_loss: 0.4198\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.4185\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3671 - val_loss: 0.4182\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3658 - val_loss: 0.4171\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.4173\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.4163\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.4154\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3612 - val_loss: 0.4146\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3603 - val_loss: 0.4135\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3592 - val_loss: 0.4138\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3583 - val_loss: 0.4125\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3572 - val_loss: 0.4128\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.4117\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.4103\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.4108\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.4117\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.4112\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.4104\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3517 - val_loss: 0.4100\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 0.4120\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.4107\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.4111\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.4089\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.4092\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.4082\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.4086\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.4077\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.4084\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train_scaled, y_train, epochs = 30,\n",
    "                   validation_data = (X_valid_scaled,y_valid),\n",
    "                   callbacks = [checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To start the TensorBoard server, one option is to open a terminal, if needed activate the virtualenv where you installed TensorBoard, go to this notebook's directory, then type:\n",
    "\n",
    "```$ tensorboard --logdir=./my_logs --port=6006```\n",
    "\n",
    "You can then open your web browser to localhost:6006 and use TensorBoard. Once you are done, press Ctrl-C in the terminal window, this will shutdown the TensorBoard server.\n",
    "\n",
    "Alternatively, you can load TensorBoard's Jupyter extension and run it like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 34236), started 0:00:50 ago. (Use '!kill 34236' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4f5210907294ccc9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4f5210907294ccc9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_logs/run_2021_05_20 - 09_33_32'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir2 = get_run_logdir()\n",
    "run_logdir2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/363 [..............................] - ETA: 0s - loss: 7.8215WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0136s). Check your callbacks.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5530 - val_loss: 3.4409\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 5292745216.0000 - val_loss: 1.3230\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3411 - val_loss: 1.3176\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3423 - val_loss: 1.3261\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3423 - val_loss: 1.3154\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3431 - val_loss: 1.3203\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3425 - val_loss: 1.3149\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3433 - val_loss: 1.3157\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3435 - val_loss: 1.3150\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3423 - val_loss: 1.3172\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3432 - val_loss: 1.3174\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3426 - val_loss: 1.3150\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3422 - val_loss: 1.3270\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3430 - val_loss: 1.3195\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3426 - val_loss: 1.3157\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3422 - val_loss: 1.3182\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3429 - val_loss: 1.3223\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3422 - val_loss: 1.3154\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3421 - val_loss: 1.3168\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3430 - val_loss: 1.3151\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3418 - val_loss: 1.3174\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3424 - val_loss: 1.3204\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3420 - val_loss: 1.3164\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3429 - val_loss: 1.3157\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3422 - val_loss: 1.3180\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3425 - val_loss: 1.3195\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3422 - val_loss: 1.3157\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3425 - val_loss: 1.3222\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3431 - val_loss: 1.3267\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3424 - val_loss: 1.3174\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir2)\n",
    "history = model.fit(X_train_scaled, y_train, epochs = 30,\n",
    "                   validation_data = (X_valid_scaled,y_valid),\n",
    "                   callbacks = [checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module tensorflow.python.keras.callbacks:\n",
      "\n",
      "__init__(self, log_dir='logs', histogram_freq=0, write_graph=True, write_images=False, update_freq='epoch', profile_batch=2, embeddings_freq=0, embeddings_metadata=None, **kwargs)\n",
      "    Initialize self.  See help(type(self)) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(keras.callbacks.TensorBoard.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Flexibility of ANN is also its main drawback. There are many hyperparameters to tweak. \n",
    "How do you know what combination of hyperparameters is best for your task?\n",
    "\n",
    "1) Try many combinations of hyperparameters. Use **RandomizedSearchCV** or **GridSearchCV** to explore the hyperparameter space. For this, we nee to wrap our keras models in objects that mimic regular scikit-learn regressors. First step is to build function that will build and compile a keras model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden = 1,n_neurons = 30,learning_rate = 3e-3,input_shape = [8]):\n",
    "    model = keras.models.Sequential()\n",
    "    options = {\"input_shape\": input_shape}\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons,activation = \"relu\",**options))\n",
    "        options = {}\n",
    "    model.add(keras.layers.Dense(1,**options))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss = \"mse\",optimizer = optimizer)\n",
    "    return model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create keras regressor based on this build_model()\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.1199 - val_loss: 0.7570\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6678 - val_loss: 0.5878\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5200 - val_loss: 0.5456\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4855 - val_loss: 0.5223\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4632 - val_loss: 0.5025\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4478 - val_loss: 0.4908\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 989us/step - loss: 0.4367 - val_loss: 0.4792\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 969us/step - loss: 0.4288 - val_loss: 0.4719\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4220 - val_loss: 0.4635\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4169 - val_loss: 0.4583\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4133 - val_loss: 0.4566\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4093 - val_loss: 0.4534\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.4062 - val_loss: 0.4520\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4034 - val_loss: 0.4502\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 986us/step - loss: 0.4005 - val_loss: 0.4474\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3979 - val_loss: 0.4451\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.3955 - val_loss: 0.4442\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.3939 - val_loss: 0.4436\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3913 - val_loss: 0.4434\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 972us/step - loss: 0.3900 - val_loss: 0.4398\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 989us/step - loss: 0.3881 - val_loss: 0.4414\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3865 - val_loss: 0.4416\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3848 - val_loss: 0.4373\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3830 - val_loss: 0.4404\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 932us/step - loss: 0.3814 - val_loss: 0.4351\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3804 - val_loss: 0.4339\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 994us/step - loss: 0.3796 - val_loss: 0.4335\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3782 - val_loss: 0.4341\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3770 - val_loss: 0.4331\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3756 - val_loss: 0.4344\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 952us/step - loss: 0.3742 - val_loss: 0.4331\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.4370\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.4318\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3711 - val_loss: 0.4320\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3705 - val_loss: 0.4317\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3697 - val_loss: 0.4327\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.4296\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.4295\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3668 - val_loss: 0.4301\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.3656 - val_loss: 0.4280\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.4267\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 988us/step - loss: 0.3646 - val_loss: 0.4275\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3633 - val_loss: 0.4307\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.4283\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.4270\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 993us/step - loss: 0.3603 - val_loss: 0.4288\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3598 - val_loss: 0.4254\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3589 - val_loss: 0.4249\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3595 - val_loss: 0.4233\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 986us/step - loss: 0.3573 - val_loss: 0.4285\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.4234\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3557 - val_loss: 0.4273\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3550 - val_loss: 0.4281\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.4246\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.4240\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3538 - val_loss: 0.4241\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 988us/step - loss: 0.3528 - val_loss: 0.4236\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3517 - val_loss: 0.4332\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3544 - val_loss: 0.4263\n",
      "162/162 [==============================] - 0s 761us/step - loss: 0.3818\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fae9e294280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train_scaled,y_train,epochs = 100,\n",
    "             validation_data = (X_valid_scaled,y_valid),\n",
    "             callbacks = [keras.callbacks.EarlyStopping(patience = 10)]\n",
    "             )\n",
    "mse_test = keras_reg.score(X_test_scaled,y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not actually want to train and evaluate a single model like this. We want to train hundreds of variants and see which one performs best on the validation set. Since there are many hyperparameters, it is preferable to use **randomized search** rather than grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] learning_rate=0.00037192261022352417, n_hidden=3, n_neurons=80 ..\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 2.9763 - val_loss: 1.6290\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3693 - val_loss: 1.0131\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9477 - val_loss: 0.8231\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7983 - val_loss: 0.7350\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7266 - val_loss: 0.6875\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6864 - val_loss: 0.6592\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6593 - val_loss: 0.6352\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6378 - val_loss: 0.6180\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6195 - val_loss: 0.6034\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6026 - val_loss: 0.5902\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5871 - val_loss: 0.5794\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5726 - val_loss: 0.5683\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5590 - val_loss: 0.5582\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5461 - val_loss: 0.5511\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5341 - val_loss: 0.5408\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5227 - val_loss: 0.5336\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5123 - val_loss: 0.5259\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5024 - val_loss: 0.5188\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4931 - val_loss: 0.5148\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4848 - val_loss: 0.5068\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4770 - val_loss: 0.5027\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4696 - val_loss: 0.4976\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4629 - val_loss: 0.4925\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4563 - val_loss: 0.4877\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4510 - val_loss: 0.4843\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4455 - val_loss: 0.4810\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4406 - val_loss: 0.4778\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4359 - val_loss: 0.4751\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4315 - val_loss: 0.4718\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4275 - val_loss: 0.4695\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4237 - val_loss: 0.4668\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4203 - val_loss: 0.4658\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4170 - val_loss: 0.4630\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4139 - val_loss: 0.4608\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.4593\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4084 - val_loss: 0.4570\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4060 - val_loss: 0.4552\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.4545\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.4516\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3993 - val_loss: 0.4522\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3972 - val_loss: 0.4506\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3953 - val_loss: 0.4501\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3936 - val_loss: 0.4479\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3917 - val_loss: 0.4480\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3902 - val_loss: 0.4466\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3884 - val_loss: 0.4448\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3871 - val_loss: 0.4444\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3856 - val_loss: 0.4430\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3841 - val_loss: 0.4428\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.4416\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3814 - val_loss: 0.4401\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3804 - val_loss: 0.4402\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3792 - val_loss: 0.4403\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3779 - val_loss: 0.4388\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3766 - val_loss: 0.4384\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 0.4379\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3747 - val_loss: 0.4376\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3735 - val_loss: 0.4368\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3726 - val_loss: 0.4359\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3715 - val_loss: 0.4353\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3709 - val_loss: 0.4352\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 0.4356\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3691 - val_loss: 0.4337\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3683 - val_loss: 0.4343\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3674 - val_loss: 0.4337\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3667 - val_loss: 0.4333\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3658 - val_loss: 0.4323\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 0.4331\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3644 - val_loss: 0.4312\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3636 - val_loss: 0.4302\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3630 - val_loss: 0.4313\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.4304\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.4307\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3610 - val_loss: 0.4294\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3604 - val_loss: 0.4291\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3596 - val_loss: 0.4283\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3592 - val_loss: 0.4285\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3585 - val_loss: 0.4294\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3579 - val_loss: 0.4291\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3572 - val_loss: 0.4277\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3567 - val_loss: 0.4283\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.4275\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.4271\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3550 - val_loss: 0.4274\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.4260\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.4271\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.4256\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3530 - val_loss: 0.4257\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3525 - val_loss: 0.4256\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3518 - val_loss: 0.4257\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.4246\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.4244\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.4247\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3501 - val_loss: 0.4235\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.4241\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.4243\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.4229\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.4227\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.4255\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.4221\n",
      "121/121 [==============================] - 0s 717us/step - loss: 0.3716\n",
      "[CV]  learning_rate=0.00037192261022352417, n_hidden=3, n_neurons=80, total=  35.1s\n",
      "[CV] learning_rate=0.00037192261022352417, n_hidden=3, n_neurons=80 ..\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   35.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 4.5724 - val_loss: 2.8102\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0576 - val_loss: 1.4288\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2137 - val_loss: 1.0718\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9441 - val_loss: 0.9013\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8085 - val_loss: 0.7952\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7303 - val_loss: 0.7297\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6823 - val_loss: 0.6860\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6499 - val_loss: 0.6525\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6257 - val_loss: 0.6278\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6059 - val_loss: 0.6072\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5885 - val_loss: 0.5908\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5730 - val_loss: 0.5748\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5592 - val_loss: 0.5625\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5464 - val_loss: 0.5529\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5345 - val_loss: 0.5423\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 0.5347\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5132 - val_loss: 0.5262\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5038 - val_loss: 0.5194\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4950 - val_loss: 0.5126\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4871 - val_loss: 0.5075\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4797 - val_loss: 0.5018\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4729 - val_loss: 0.4978\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4667 - val_loss: 0.4943\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4611 - val_loss: 0.4897\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4559 - val_loss: 0.4862\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4511 - val_loss: 0.4832\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4467 - val_loss: 0.4813\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4426 - val_loss: 0.4781\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4386 - val_loss: 0.4780\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4357 - val_loss: 0.4736\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4324 - val_loss: 0.4710\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.4696\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4267 - val_loss: 0.4681\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4240 - val_loss: 0.4676\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.4657\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.4626\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4170 - val_loss: 0.4621\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4148 - val_loss: 0.4604\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4127 - val_loss: 0.4586\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.4576\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4088 - val_loss: 0.4568\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4072 - val_loss: 0.4559\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4054 - val_loss: 0.4532\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.4526\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4021 - val_loss: 0.4524\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4006 - val_loss: 0.4492\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.4487\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3977 - val_loss: 0.4472\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3962 - val_loss: 0.4468\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3950 - val_loss: 0.4456\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3934 - val_loss: 0.4429\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3925 - val_loss: 0.4432\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3912 - val_loss: 0.4419\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3900 - val_loss: 0.4415\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3889 - val_loss: 0.4410\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3878 - val_loss: 0.4399\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3865 - val_loss: 0.4379\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3857 - val_loss: 0.4383\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3846 - val_loss: 0.4374\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3834 - val_loss: 0.4360\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3825 - val_loss: 0.4365\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3816 - val_loss: 0.4364\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.4341\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3798 - val_loss: 0.4340\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.4352\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3780 - val_loss: 0.4348\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.4324\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3762 - val_loss: 0.4335\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3755 - val_loss: 0.4325\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3746 - val_loss: 0.4297\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.4296\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.4299\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3724 - val_loss: 0.4287\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3717 - val_loss: 0.4291\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3711 - val_loss: 0.4286\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3702 - val_loss: 0.4287\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3694 - val_loss: 0.4274\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3689 - val_loss: 0.4278\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3682 - val_loss: 0.4275\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3675 - val_loss: 0.4269\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3668 - val_loss: 0.4268\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3663 - val_loss: 0.4260\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.4274\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 0.4259\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.4256\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.4249\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.4248\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3628 - val_loss: 0.4247\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.4250\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3617 - val_loss: 0.4245\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3612 - val_loss: 0.4237\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3608 - val_loss: 0.4241\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.4237\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3597 - val_loss: 0.4235\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3592 - val_loss: 0.4238\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3588 - val_loss: 0.4234\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3582 - val_loss: 0.4223\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3577 - val_loss: 0.4216\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3574 - val_loss: 0.4225\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.4221\n",
      "121/121 [==============================] - 0s 793us/step - loss: 0.3699\n",
      "[CV]  learning_rate=0.00037192261022352417, n_hidden=3, n_neurons=80, total=  34.5s\n",
      "[CV] learning_rate=0.00037192261022352417, n_hidden=3, n_neurons=80 ..\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.3295 - val_loss: 1.9345\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5448 - val_loss: 1.1609\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0275 - val_loss: 0.9179\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8185 - val_loss: 0.7929\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7155 - val_loss: 0.7186\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6604 - val_loss: 0.6752\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6277 - val_loss: 0.6497\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6065 - val_loss: 0.6274\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5908 - val_loss: 0.6135\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5774 - val_loss: 0.6012\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5651 - val_loss: 0.5898\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5537 - val_loss: 0.5784\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5430 - val_loss: 0.5693\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5328 - val_loss: 0.5615\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 0.5538\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5144 - val_loss: 0.5443\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5060 - val_loss: 0.5369\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4980 - val_loss: 0.5294\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4906 - val_loss: 0.5232\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4836 - val_loss: 0.5179\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4769 - val_loss: 0.5124\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4706 - val_loss: 0.5076\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4647 - val_loss: 0.5022\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4590 - val_loss: 0.4998\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4539 - val_loss: 0.4940\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4489 - val_loss: 0.4897\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4443 - val_loss: 0.4856\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4400 - val_loss: 0.4820\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4358 - val_loss: 0.4810\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4322 - val_loss: 0.4750\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4286 - val_loss: 0.4718\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4252 - val_loss: 0.4708\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4220 - val_loss: 0.4662\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.4669\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4163 - val_loss: 0.4633\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4136 - val_loss: 0.4602\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4113 - val_loss: 0.4591\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4089 - val_loss: 0.4567\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.4552\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4045 - val_loss: 0.4526\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4023 - val_loss: 0.4537\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4007 - val_loss: 0.4509\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3987 - val_loss: 0.4483\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3971 - val_loss: 0.4480\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3954 - val_loss: 0.4472\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3938 - val_loss: 0.4442\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3922 - val_loss: 0.4432\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3909 - val_loss: 0.4424\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3893 - val_loss: 0.4411\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3881 - val_loss: 0.4405\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3866 - val_loss: 0.4390\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3854 - val_loss: 0.4383\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3842 - val_loss: 0.4374\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3831 - val_loss: 0.4366\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3818 - val_loss: 0.4369\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.4357\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3796 - val_loss: 0.4343\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.4338\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3776 - val_loss: 0.4327\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3765 - val_loss: 0.4312\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.4323\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3747 - val_loss: 0.4320\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.4300\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.4294\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3723 - val_loss: 0.4293\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3713 - val_loss: 0.4299\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3706 - val_loss: 0.4281\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.4275\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.4275\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3682 - val_loss: 0.4267\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3674 - val_loss: 0.4252\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3668 - val_loss: 0.4261\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3659 - val_loss: 0.4244\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3654 - val_loss: 0.4245\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3648 - val_loss: 0.4249\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.4240\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.4231\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3629 - val_loss: 0.4227\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.4221\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.4241\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3611 - val_loss: 0.4226\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.4217\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3600 - val_loss: 0.4219\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3594 - val_loss: 0.4213\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3588 - val_loss: 0.4198\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.4206\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3577 - val_loss: 0.4207\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3573 - val_loss: 0.4201\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3568 - val_loss: 0.4205\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.4187\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3558 - val_loss: 0.4184\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3554 - val_loss: 0.4190\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3548 - val_loss: 0.4185\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.4182\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.4184\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.4181\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3531 - val_loss: 0.4170\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3526 - val_loss: 0.4175\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.4168\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.4177\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3528\n",
      "[CV]  learning_rate=0.00037192261022352417, n_hidden=3, n_neurons=80, total=  34.1s\n",
      "[CV] learning_rate=0.0008763224455697141, n_hidden=1, n_neurons=47 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.0810 - val_loss: 1.0760\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9132 - val_loss: 0.7870\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7509 - val_loss: 0.7243\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6988 - val_loss: 0.6875\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6638 - val_loss: 0.6595\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6359 - val_loss: 0.6365\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6126 - val_loss: 0.6161\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5916 - val_loss: 0.5979\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5737 - val_loss: 0.5848\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5562 - val_loss: 0.5702\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5417 - val_loss: 0.5583\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5293 - val_loss: 0.5505\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5166 - val_loss: 0.5391\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5062 - val_loss: 0.5323\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4971 - val_loss: 0.5248\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4876 - val_loss: 0.5173\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4801 - val_loss: 0.5113\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4727 - val_loss: 0.5057\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4661 - val_loss: 0.5026\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4603 - val_loss: 0.4961\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4549 - val_loss: 0.4940\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4496 - val_loss: 0.4898\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4451 - val_loss: 0.4850\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4408 - val_loss: 0.4815\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4372 - val_loss: 0.4791\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4334 - val_loss: 0.4777\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4302 - val_loss: 0.4738\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4275 - val_loss: 0.4728\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.4691\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4220 - val_loss: 0.4681\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4196 - val_loss: 0.4655\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4173 - val_loss: 0.4659\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.4627\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4133 - val_loss: 0.4610\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4114 - val_loss: 0.4601\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.4580\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.4565\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4064 - val_loss: 0.4562\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4048 - val_loss: 0.4534\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4034 - val_loss: 0.4543\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4020 - val_loss: 0.4536\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4007 - val_loss: 0.4526\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3995 - val_loss: 0.4507\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3982 - val_loss: 0.4505\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3971 - val_loss: 0.4491\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3959 - val_loss: 0.4482\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3949 - val_loss: 0.4477\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3938 - val_loss: 0.4466\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3929 - val_loss: 0.4459\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3919 - val_loss: 0.4454\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3909 - val_loss: 0.4439\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3901 - val_loss: 0.4441\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3892 - val_loss: 0.4441\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3884 - val_loss: 0.4428\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3874 - val_loss: 0.4430\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3867 - val_loss: 0.4427\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3859 - val_loss: 0.4422\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3850 - val_loss: 0.4410\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3843 - val_loss: 0.4404\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3835 - val_loss: 0.4402\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.4396\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3821 - val_loss: 0.4407\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3815 - val_loss: 0.4383\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.4385\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3802 - val_loss: 0.4393\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.4386\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3789 - val_loss: 0.4377\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3784 - val_loss: 0.4386\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3777 - val_loss: 0.4366\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3772 - val_loss: 0.4354\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3766 - val_loss: 0.4366\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.4357\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3756 - val_loss: 0.4360\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3749 - val_loss: 0.4360\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.4366\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3738 - val_loss: 0.4347\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3735 - val_loss: 0.4347\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3728 - val_loss: 0.4362\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3723 - val_loss: 0.4347\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3719 - val_loss: 0.4341\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3714 - val_loss: 0.4357\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3710 - val_loss: 0.4344\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3706 - val_loss: 0.4338\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3701 - val_loss: 0.4337\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3697 - val_loss: 0.4333\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.4349\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.4327\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.4337\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3680 - val_loss: 0.4327\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3676 - val_loss: 0.4338\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3674 - val_loss: 0.4322\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3670 - val_loss: 0.4328\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3666 - val_loss: 0.4324\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3663 - val_loss: 0.4312\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3659 - val_loss: 0.4326\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3654 - val_loss: 0.4320\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 0.4308\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3649 - val_loss: 0.4311\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3643 - val_loss: 0.4340\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3644 - val_loss: 0.4309\n",
      "121/121 [==============================] - 0s 706us/step - loss: 0.3782\n",
      "[CV]  learning_rate=0.0008763224455697141, n_hidden=1, n_neurons=47, total=  29.2s\n",
      "[CV] learning_rate=0.0008763224455697141, n_hidden=1, n_neurons=47 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4823 - val_loss: 1.0497\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8435 - val_loss: 0.7583\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7054 - val_loss: 0.7037\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6632 - val_loss: 0.6726\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6339 - val_loss: 0.6488\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6098 - val_loss: 0.6290\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5890 - val_loss: 0.6135\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5706 - val_loss: 0.5983\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5544 - val_loss: 0.5862\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5402 - val_loss: 0.5752\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5276 - val_loss: 0.5648\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5162 - val_loss: 0.5562\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5062 - val_loss: 0.5479\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4971 - val_loss: 0.5410\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4890 - val_loss: 0.5332\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4816 - val_loss: 0.5274\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4750 - val_loss: 0.5208\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4690 - val_loss: 0.5153\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4634 - val_loss: 0.5107\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.5073\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4541 - val_loss: 0.5028\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4501 - val_loss: 0.4987\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4463 - val_loss: 0.4944\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4429 - val_loss: 0.4909\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4397 - val_loss: 0.4872\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4368 - val_loss: 0.4842\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4339 - val_loss: 0.4816\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4314 - val_loss: 0.4781\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4287 - val_loss: 0.4765\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4269 - val_loss: 0.4735\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4246 - val_loss: 0.4710\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4226 - val_loss: 0.4696\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4208 - val_loss: 0.4680\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4189 - val_loss: 0.4663\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4173 - val_loss: 0.4644\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4156 - val_loss: 0.4628\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4142 - val_loss: 0.4611\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4127 - val_loss: 0.4597\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4112 - val_loss: 0.4573\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4101 - val_loss: 0.4571\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4087 - val_loss: 0.4566\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4075 - val_loss: 0.4553\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4063 - val_loss: 0.4538\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4052 - val_loss: 0.4525\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.4529\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4031 - val_loss: 0.4505\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4020 - val_loss: 0.4499\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4010 - val_loss: 0.4494\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4001 - val_loss: 0.4486\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3992 - val_loss: 0.4475\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3981 - val_loss: 0.4464\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3973 - val_loss: 0.4460\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3964 - val_loss: 0.4445\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3955 - val_loss: 0.4450\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3947 - val_loss: 0.4444\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3938 - val_loss: 0.4428\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3929 - val_loss: 0.4414\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3923 - val_loss: 0.4419\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3915 - val_loss: 0.4414\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3905 - val_loss: 0.4404\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3899 - val_loss: 0.4401\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3893 - val_loss: 0.4409\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3885 - val_loss: 0.4392\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3879 - val_loss: 0.4389\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3871 - val_loss: 0.4399\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3865 - val_loss: 0.4395\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3860 - val_loss: 0.4384\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3852 - val_loss: 0.4386\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3846 - val_loss: 0.4379\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3840 - val_loss: 0.4355\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3835 - val_loss: 0.4359\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.4366\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.4351\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3818 - val_loss: 0.4352\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3813 - val_loss: 0.4355\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.4352\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3801 - val_loss: 0.4343\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.4346\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3792 - val_loss: 0.4345\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.4334\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3781 - val_loss: 0.4339\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3777 - val_loss: 0.4326\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3772 - val_loss: 0.4342\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3768 - val_loss: 0.4327\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3763 - val_loss: 0.4326\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3759 - val_loss: 0.4320\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3755 - val_loss: 0.4319\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3749 - val_loss: 0.4323\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3746 - val_loss: 0.4320\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.4318\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3738 - val_loss: 0.4306\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3734 - val_loss: 0.4309\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.4306\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3726 - val_loss: 0.4299\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3722 - val_loss: 0.4306\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3719 - val_loss: 0.4305\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3714 - val_loss: 0.4286\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3711 - val_loss: 0.4281\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.4290\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3704 - val_loss: 0.4285\n",
      "121/121 [==============================] - 0s 722us/step - loss: 0.3919\n",
      "[CV]  learning_rate=0.0008763224455697141, n_hidden=1, n_neurons=47, total=  29.7s\n",
      "[CV] learning_rate=0.0008763224455697141, n_hidden=1, n_neurons=47 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.0643 - val_loss: 1.6711\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2626 - val_loss: 0.9074\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8097 - val_loss: 0.7144\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6837 - val_loss: 0.6464\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6349 - val_loss: 0.6148\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6077 - val_loss: 0.5919\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5881 - val_loss: 0.5782\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5716 - val_loss: 0.5613\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5587 - val_loss: 0.5529\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5455 - val_loss: 0.5440\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5335 - val_loss: 0.5319\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 0.5249\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5138 - val_loss: 0.5184\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5048 - val_loss: 0.5108\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4965 - val_loss: 0.5049\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4887 - val_loss: 0.4966\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4819 - val_loss: 0.4932\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4751 - val_loss: 0.4882\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4688 - val_loss: 0.4826\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4641 - val_loss: 0.4806\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.4772\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4543 - val_loss: 0.4744\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4504 - val_loss: 0.4736\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4466 - val_loss: 0.4720\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4433 - val_loss: 0.4679\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4403 - val_loss: 0.4674\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4374 - val_loss: 0.4637\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4352 - val_loss: 0.4631\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4325 - val_loss: 0.4627\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4605\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4283 - val_loss: 0.4588\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.4585\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.4564\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.4575\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4213 - val_loss: 0.4557\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4196 - val_loss: 0.4549\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4183 - val_loss: 0.4538\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4168 - val_loss: 0.4530\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.4522\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4139 - val_loss: 0.4512\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4126 - val_loss: 0.4519\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4114 - val_loss: 0.4509\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4100 - val_loss: 0.4487\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4091 - val_loss: 0.4491\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4079 - val_loss: 0.4485\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.4469\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4057 - val_loss: 0.4461\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4048 - val_loss: 0.4456\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4037 - val_loss: 0.4451\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4028 - val_loss: 0.4451\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4017 - val_loss: 0.4446\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 0.4444\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4000 - val_loss: 0.4430\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.4432\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3982 - val_loss: 0.4430\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3972 - val_loss: 0.4427\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3964 - val_loss: 0.4415\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3958 - val_loss: 0.4410\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3949 - val_loss: 0.4405\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3941 - val_loss: 0.4395\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3934 - val_loss: 0.4406\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.4411\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.4391\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3913 - val_loss: 0.4390\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3906 - val_loss: 0.4383\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3899 - val_loss: 0.4392\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3894 - val_loss: 0.4375\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3887 - val_loss: 0.4379\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3881 - val_loss: 0.4377\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3874 - val_loss: 0.4367\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3868 - val_loss: 0.4353\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.4366\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3856 - val_loss: 0.4350\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3851 - val_loss: 0.4353\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3845 - val_loss: 0.4356\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3838 - val_loss: 0.4359\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3834 - val_loss: 0.4347\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.4343\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.4338\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.4349\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3813 - val_loss: 0.4331\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.4335\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3803 - val_loss: 0.4336\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.4333\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3792 - val_loss: 0.4316\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3788 - val_loss: 0.4323\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3782 - val_loss: 0.4325\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3779 - val_loss: 0.4318\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.4328\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3770 - val_loss: 0.4308\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3763 - val_loss: 0.4313\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3761 - val_loss: 0.4301\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3755 - val_loss: 0.4308\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.4301\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3746 - val_loss: 0.4307\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.4310\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3738 - val_loss: 0.4292\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3734 - val_loss: 0.4291\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3729 - val_loss: 0.4295\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3727 - val_loss: 0.4293\n",
      "121/121 [==============================] - 0s 739us/step - loss: 0.3694\n",
      "[CV]  learning_rate=0.0008763224455697141, n_hidden=1, n_neurons=47, total=  30.2s\n",
      "[CV] learning_rate=0.0051747964719537, n_hidden=3, n_neurons=55 ......\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2976 - val_loss: 0.7112\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6853 - val_loss: 0.6266\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5626 - val_loss: 0.5035\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4459 - val_loss: 0.4663\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.4430\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.4382\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.4340\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3649 - val_loss: 0.4231\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3590 - val_loss: 0.4363\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.4195\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.4218\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.4286\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3434 - val_loss: 0.4167\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3418 - val_loss: 0.4156\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3385 - val_loss: 0.4125\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3367 - val_loss: 0.4150\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3341 - val_loss: 0.4106\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3329 - val_loss: 0.4163\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3296 - val_loss: 0.4088\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3281 - val_loss: 0.4109\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3263 - val_loss: 0.4094\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3250 - val_loss: 0.4095\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3227 - val_loss: 0.4040\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3219 - val_loss: 0.4111\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3186 - val_loss: 0.4093\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3174 - val_loss: 0.4068\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3164 - val_loss: 0.4029\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3148 - val_loss: 0.4087\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3126 - val_loss: 0.4004\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3108 - val_loss: 0.4045\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3096 - val_loss: 0.3965\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3085 - val_loss: 0.3993\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3063 - val_loss: 0.3987\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3046 - val_loss: 0.4017\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3035 - val_loss: 0.3985\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3026 - val_loss: 0.3950\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3012 - val_loss: 0.3915\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2996 - val_loss: 0.4010\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2988 - val_loss: 0.3951\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2975 - val_loss: 0.3921\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2964 - val_loss: 0.3917\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2944 - val_loss: 0.3972\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2944 - val_loss: 0.3964\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2927 - val_loss: 0.3998\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2922 - val_loss: 0.3923\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2911 - val_loss: 0.3887\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2895 - val_loss: 0.3900\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2895 - val_loss: 0.3867\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2878 - val_loss: 0.3909\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2876 - val_loss: 0.3933\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2858 - val_loss: 0.4106\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2848 - val_loss: 0.3968\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2846 - val_loss: 0.3901\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2826 - val_loss: 0.4049\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2817 - val_loss: 0.3919\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2819 - val_loss: 0.3904\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2798 - val_loss: 0.3950\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2801 - val_loss: 0.3938\n",
      "121/121 [==============================] - 0s 780us/step - loss: 0.3199\n",
      "[CV]  learning_rate=0.0051747964719537, n_hidden=3, n_neurons=55, total=  19.7s\n",
      "[CV] learning_rate=0.0051747964719537, n_hidden=3, n_neurons=55 ......\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0667 - val_loss: 0.6168\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5782 - val_loss: 0.5397\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4926 - val_loss: 0.4902\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4391 - val_loss: 0.4567\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.4432\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.4307\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.4271\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3679 - val_loss: 0.4193\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3601 - val_loss: 0.4232\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.4137\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 0.4180\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.4143\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3442 - val_loss: 0.4141\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3429 - val_loss: 0.4103\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3398 - val_loss: 0.4079\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3361 - val_loss: 0.4065\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3348 - val_loss: 0.4059\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3326 - val_loss: 0.4245\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3296 - val_loss: 0.4067\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3276 - val_loss: 0.4089\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3272 - val_loss: 0.4095\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3248 - val_loss: 0.4056\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3223 - val_loss: 0.4217\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3222 - val_loss: 0.4115\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3188 - val_loss: 0.4174\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3177 - val_loss: 0.4048\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3158 - val_loss: 0.4085\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3152 - val_loss: 0.4022\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3147 - val_loss: 0.4037\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3145 - val_loss: 0.4096\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3106 - val_loss: 0.4052\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3092 - val_loss: 0.4098\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3082 - val_loss: 0.4038\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3059 - val_loss: 0.4035\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3049 - val_loss: 0.3996\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3041 - val_loss: 0.4060\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3027 - val_loss: 0.3938\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3010 - val_loss: 0.4124\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3007 - val_loss: 0.3979\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2992 - val_loss: 0.4027\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2970 - val_loss: 0.4147\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2962 - val_loss: 0.4005\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2956 - val_loss: 0.4005\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2947 - val_loss: 0.4118\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2933 - val_loss: 0.4117\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2935 - val_loss: 0.3953\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2917 - val_loss: 0.3984\n",
      "121/121 [==============================] - 0s 905us/step - loss: 0.3145\n",
      "[CV]  learning_rate=0.0051747964719537, n_hidden=3, n_neurons=55, total=  16.9s\n",
      "[CV] learning_rate=0.0051747964719537, n_hidden=3, n_neurons=55 ......\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1737 - val_loss: 0.6844\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6127 - val_loss: 0.5713\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5108 - val_loss: 0.5022\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4515 - val_loss: 0.4741\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4136 - val_loss: 0.4518\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3968 - val_loss: 0.4470\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3857 - val_loss: 0.4326\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3767 - val_loss: 0.4275\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3694 - val_loss: 0.4248\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3650 - val_loss: 0.4183\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3599 - val_loss: 0.4200\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3559 - val_loss: 0.4161\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.4131\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.4160\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3451 - val_loss: 0.4100\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3409 - val_loss: 0.4057\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3386 - val_loss: 0.4028\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3352 - val_loss: 0.4110\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3333 - val_loss: 0.4064\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3304 - val_loss: 0.4043\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3287 - val_loss: 0.4014\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3264 - val_loss: 0.4031\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.4057\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3228 - val_loss: 0.4038\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3212 - val_loss: 0.3994\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3184 - val_loss: 0.3919\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.4005\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3163 - val_loss: 0.4005\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3168 - val_loss: 0.3929\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3158 - val_loss: 0.4032\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3114 - val_loss: 0.3915\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3101 - val_loss: 0.3963\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3088 - val_loss: 0.3932\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3073 - val_loss: 0.3888\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3055 - val_loss: 0.4102\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3031 - val_loss: 0.3988\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3034 - val_loss: 0.3938\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3030 - val_loss: 0.4014\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3022 - val_loss: 0.3865\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3006 - val_loss: 0.3905\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2996 - val_loss: 0.3967\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2980 - val_loss: 0.3969\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2980 - val_loss: 0.3929\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2966 - val_loss: 0.3963\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2961 - val_loss: 0.4114\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2942 - val_loss: 0.4024\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2926 - val_loss: 0.3927\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2933 - val_loss: 0.3858\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2909 - val_loss: 0.3969\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2916 - val_loss: 0.3828\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2900 - val_loss: 0.3931\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2900 - val_loss: 0.3885\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2891 - val_loss: 0.3927\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2884 - val_loss: 0.3931\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2870 - val_loss: 0.3872\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2860 - val_loss: 0.3902\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2868 - val_loss: 0.3919\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2876 - val_loss: 0.3900\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2850 - val_loss: 0.3825\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2845 - val_loss: 0.3904\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2835 - val_loss: 0.4018\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2823 - val_loss: 0.3892\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2818 - val_loss: 0.4071\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2825 - val_loss: 0.3979\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2815 - val_loss: 0.3934\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2806 - val_loss: 0.3940\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2794 - val_loss: 0.3872\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2807 - val_loss: 0.3901\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2785 - val_loss: 0.4002\n",
      "121/121 [==============================] - 0s 916us/step - loss: 0.2934\n",
      "[CV]  learning_rate=0.0051747964719537, n_hidden=3, n_neurons=55, total=  24.2s\n",
      "[CV] learning_rate=0.027770456976200626, n_hidden=0, n_neurons=3 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9613 - val_loss: 13.7797\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 7.4969 - val_loss: 11.8600\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 8.4882 - val_loss: 45.5331\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.3493 - val_loss: 41.6302\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 12.6452 - val_loss: 134.7210\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 609.6024 - val_loss: 261.7259\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 188.0302 - val_loss: 570.7606\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2152.9268 - val_loss: 1136.4449\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1669.3958 - val_loss: 2547.6179\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 14201.2451 - val_loss: 5804.3398\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6814.2212 - val_loss: 11670.2871\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1320.4941 - val_loss: 23122.9629\n",
      "121/121 [==============================] - 0s 647us/step - loss: 6021.9429\n",
      "[CV]  learning_rate=0.027770456976200626, n_hidden=0, n_neurons=3, total=   4.0s\n",
      "[CV] learning_rate=0.027770456976200626, n_hidden=0, n_neurons=3 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8021 - val_loss: 0.5791\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5208 - val_loss: 0.6847\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5085 - val_loss: 0.7146\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5188 - val_loss: 0.6832\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5188 - val_loss: 0.6796\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5154 - val_loss: 0.6677\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5215 - val_loss: 0.7000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5224 - val_loss: 0.6994\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5137 - val_loss: 0.6668\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5187 - val_loss: 0.5253\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5141 - val_loss: 0.6675\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5099 - val_loss: 0.7329\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5153 - val_loss: 0.7564\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5763 - val_loss: 0.5266\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5130 - val_loss: 0.6347\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5128 - val_loss: 0.6848\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5121 - val_loss: 0.5373\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.5173 - val_loss: 0.6268\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5145 - val_loss: 0.6831\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5057 - val_loss: 0.5497\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.6937\n",
      "[CV]  learning_rate=0.027770456976200626, n_hidden=0, n_neurons=3, total=   6.4s\n",
      "[CV] learning_rate=0.027770456976200626, n_hidden=0, n_neurons=3 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1685 - val_loss: 5.9517\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8667 - val_loss: 0.5027\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5935 - val_loss: 10.7385\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 55.6775 - val_loss: 1.6090\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7632 - val_loss: 27.8409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1478 - val_loss: 0.8874\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8198 - val_loss: 1.0090\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2233 - val_loss: 0.7307\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0833 - val_loss: 0.6678\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6072 - val_loss: 4.0989\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 23.2656 - val_loss: 1.0256\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 1.7419 - val_loss: 2.1421\n",
      "121/121 [==============================] - 0s 729us/step - loss: 0.9138\n",
      "[CV]  learning_rate=0.027770456976200626, n_hidden=0, n_neurons=3, total=   3.9s\n",
      "[CV] learning_rate=0.01573990360087585, n_hidden=2, n_neurons=21 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7989 - val_loss: 0.5587\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4695 - val_loss: 0.4895\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4147 - val_loss: 0.4610\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3954 - val_loss: 0.4431\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3869 - val_loss: 0.4447\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.4491\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3724 - val_loss: 0.4407\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3654 - val_loss: 0.4416\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.4570\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 0.4408\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3600 - val_loss: 0.4451\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3627 - val_loss: 0.4495\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3621 - val_loss: 0.4377\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.4348\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.4332\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.4280\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3449 - val_loss: 0.4272\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3430 - val_loss: 0.4311\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3417 - val_loss: 0.4251\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3380 - val_loss: 0.4263\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3383 - val_loss: 0.4240\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3361 - val_loss: 0.4217\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 0.4362\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3344 - val_loss: 0.4367\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3286 - val_loss: 0.4374\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3263 - val_loss: 0.4250\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3271 - val_loss: 0.4303\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3305 - val_loss: 0.4207\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3241 - val_loss: 0.4117\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3212 - val_loss: 0.4207\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3210 - val_loss: 0.4078\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3204 - val_loss: 0.4105\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3180 - val_loss: 0.4040\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3145 - val_loss: 0.4233\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3134 - val_loss: 0.4106\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3114 - val_loss: 0.4085\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3109 - val_loss: 0.4046\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3085 - val_loss: 0.4058\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3082 - val_loss: 0.4134\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3068 - val_loss: 0.4002\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3071 - val_loss: 0.4041\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3032 - val_loss: 0.4084\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3023 - val_loss: 0.4106\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2998 - val_loss: 0.4135\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3022 - val_loss: 0.4124\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2978 - val_loss: 0.3970\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2959 - val_loss: 0.3898\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2968 - val_loss: 0.3912\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2932 - val_loss: 0.3978\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2918 - val_loss: 0.4010\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2922 - val_loss: 0.4434\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2928 - val_loss: 0.4006\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2889 - val_loss: 0.4016\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2858 - val_loss: 0.4210\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2850 - val_loss: 0.3992\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2856 - val_loss: 0.3979\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2822 - val_loss: 0.4086\n",
      "121/121 [==============================] - 0s 799us/step - loss: 0.3214\n",
      "[CV]  learning_rate=0.01573990360087585, n_hidden=2, n_neurons=21, total=  18.5s\n",
      "[CV] learning_rate=0.01573990360087585, n_hidden=2, n_neurons=21 .....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7219 - val_loss: 0.4969\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4342 - val_loss: 0.4626\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.4692\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3829 - val_loss: 0.4395\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3766 - val_loss: 0.4408\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3688 - val_loss: 0.4308\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.4271\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3611 - val_loss: 0.4305\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3580 - val_loss: 0.4439\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3568 - val_loss: 0.4138\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3525 - val_loss: 0.4265\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3501 - val_loss: 0.4221\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.4312\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.4220\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.4152\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.4124\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3444 - val_loss: 0.4065\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3436 - val_loss: 0.4166\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3413 - val_loss: 0.4192\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3390 - val_loss: 0.4105\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3398 - val_loss: 0.4067\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3379 - val_loss: 0.4088\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.4281\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3361 - val_loss: 0.4145\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.4208\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3312 - val_loss: 0.4065\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3302 - val_loss: 0.4082\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3298 - val_loss: 0.4006\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3288 - val_loss: 0.4040\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3287 - val_loss: 0.4194\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 0.4118\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3241 - val_loss: 0.4024\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3223 - val_loss: 0.4050\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.4023\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3201 - val_loss: 0.3979\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3188 - val_loss: 0.4029\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3168 - val_loss: 0.3915\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3154 - val_loss: 0.4196\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3144 - val_loss: 0.3931\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3123 - val_loss: 0.3978\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3106 - val_loss: 0.4096\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3093 - val_loss: 0.3922\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3084 - val_loss: 0.3967\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3085 - val_loss: 0.4101\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3057 - val_loss: 0.4161\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3053 - val_loss: 0.3908\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3044 - val_loss: 0.3874\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3035 - val_loss: 0.3980\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3012 - val_loss: 0.4144\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3030 - val_loss: 0.4145\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2996 - val_loss: 0.4348\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3007 - val_loss: 0.3941\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2982 - val_loss: 0.3892\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2984 - val_loss: 0.3981\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2962 - val_loss: 0.3913\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2978 - val_loss: 0.4134\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2950 - val_loss: 0.4088\n",
      "121/121 [==============================] - 0s 821us/step - loss: 0.3329\n",
      "[CV]  learning_rate=0.01573990360087585, n_hidden=2, n_neurons=21, total=  17.5s\n",
      "[CV] learning_rate=0.01573990360087585, n_hidden=2, n_neurons=21 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7898 - val_loss: 0.5818\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4804 - val_loss: 0.4770\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4410 - val_loss: 0.4678\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4039 - val_loss: 0.4351\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3834 - val_loss: 0.4340\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3763 - val_loss: 0.4404\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3720 - val_loss: 0.4241\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.4272\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.4310\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3603 - val_loss: 0.4257\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3568 - val_loss: 0.4207\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3552 - val_loss: 0.4273\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.4243\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.4233\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.4226\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3448 - val_loss: 0.4163\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3428 - val_loss: 0.4075\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3427 - val_loss: 0.4190\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3393 - val_loss: 0.4148\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3363 - val_loss: 0.4222\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3344 - val_loss: 0.4121\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.4110\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3304 - val_loss: 0.4210\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3287 - val_loss: 0.4104\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3270 - val_loss: 0.4061\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3249 - val_loss: 0.3945\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3226 - val_loss: 0.4101\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3217 - val_loss: 0.4073\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3282 - val_loss: 0.3963\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3255 - val_loss: 0.4183\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3179 - val_loss: 0.3979\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3165 - val_loss: 0.4005\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3158 - val_loss: 0.4033\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3139 - val_loss: 0.3967\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3134 - val_loss: 0.4181\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3113 - val_loss: 0.4041\n",
      "121/121 [==============================] - 0s 816us/step - loss: 0.3186\n",
      "[CV]  learning_rate=0.01573990360087585, n_hidden=2, n_neurons=21, total=  11.7s\n",
      "[CV] learning_rate=0.002388469823418883, n_hidden=1, n_neurons=4 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.0784 - val_loss: 1.1980\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8372 - val_loss: 0.7226\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6486 - val_loss: 0.6471\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5868 - val_loss: 0.6008\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5468 - val_loss: 0.5694\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5233 - val_loss: 0.5481\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5084 - val_loss: 0.5362\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4964 - val_loss: 0.5219\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4870 - val_loss: 0.5152\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4790 - val_loss: 0.5047\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4723 - val_loss: 0.4990\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4666 - val_loss: 0.4960\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4618 - val_loss: 0.4895\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4578 - val_loss: 0.4866\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4542 - val_loss: 0.4824\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4511 - val_loss: 0.4812\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4485 - val_loss: 0.4794\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4458 - val_loss: 0.4785\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4422 - val_loss: 0.4737\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4405 - val_loss: 0.4731\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4383 - val_loss: 0.4738\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4355 - val_loss: 0.4722\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4343 - val_loss: 0.4718\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4323 - val_loss: 0.4733\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4311 - val_loss: 0.4693\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4297 - val_loss: 0.4687\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4284 - val_loss: 0.4684\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4268 - val_loss: 0.4663\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4257 - val_loss: 0.4640\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4249 - val_loss: 0.4661\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4235 - val_loss: 0.4644\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4231 - val_loss: 0.4662\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4219 - val_loss: 0.4629\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4210 - val_loss: 0.4623\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4206 - val_loss: 0.4621\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4195 - val_loss: 0.4630\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.4615\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4183 - val_loss: 0.4618\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4174 - val_loss: 0.4586\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4176 - val_loss: 0.4608\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4170 - val_loss: 0.4633\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4161 - val_loss: 0.4622\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4163 - val_loss: 0.4615\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4156 - val_loss: 0.4603\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.4596\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4146 - val_loss: 0.4620\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4147 - val_loss: 0.4593\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.4145 - val_loss: 0.4597\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.4140 - val_loss: 0.4593\n",
      "121/121 [==============================] - 0s 606us/step - loss: 0.4301\n",
      "[CV]  learning_rate=0.002388469823418883, n_hidden=1, n_neurons=4, total=  14.1s\n",
      "[CV] learning_rate=0.002388469823418883, n_hidden=1, n_neurons=4 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2774 - val_loss: 1.0045\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8215 - val_loss: 0.6947\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6630 - val_loss: 0.6466\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6167 - val_loss: 0.6216\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5823 - val_loss: 0.6015\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5538 - val_loss: 0.5845\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5309 - val_loss: 0.5692\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5123 - val_loss: 0.5526\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4971 - val_loss: 0.5410\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4846 - val_loss: 0.5293\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4745 - val_loss: 0.5192\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4661 - val_loss: 0.5087\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4592 - val_loss: 0.5008\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4538 - val_loss: 0.4962\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4491 - val_loss: 0.4891\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4454 - val_loss: 0.4855\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4424 - val_loss: 0.4807\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4399 - val_loss: 0.4776\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4372 - val_loss: 0.4736\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4353 - val_loss: 0.4729\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4697\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4323 - val_loss: 0.4675\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4309 - val_loss: 0.4651\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4301 - val_loss: 0.4636\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4290 - val_loss: 0.4615\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4280 - val_loss: 0.4604\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4272 - val_loss: 0.4593\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4267 - val_loss: 0.4585\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4254 - val_loss: 0.4583\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.4573\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4246 - val_loss: 0.4567\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4241 - val_loss: 0.4561\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4237 - val_loss: 0.4556\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4231 - val_loss: 0.4546\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.4539\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4224 - val_loss: 0.4542\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4225 - val_loss: 0.4541\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4217 - val_loss: 0.4542\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4217 - val_loss: 0.4525\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4214 - val_loss: 0.4526\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4209 - val_loss: 0.4523\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4207 - val_loss: 0.4530\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4204 - val_loss: 0.4526\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4203 - val_loss: 0.4524\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4200 - val_loss: 0.4528\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4197 - val_loss: 0.4511\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4193 - val_loss: 0.4516\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.4192 - val_loss: 0.4516\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.4508\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4188 - val_loss: 0.4500\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.4514\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.4498\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.4494\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4177 - val_loss: 0.4504\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.4496\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4172 - val_loss: 0.4486\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4171 - val_loss: 0.4486\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4171 - val_loss: 0.4486\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4167 - val_loss: 0.4489\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.4489\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4159 - val_loss: 0.4493\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4157 - val_loss: 0.4500\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4156 - val_loss: 0.4483\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4154 - val_loss: 0.4485\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4149 - val_loss: 0.4493\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4150 - val_loss: 0.4486\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4149 - val_loss: 0.4483\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4143 - val_loss: 0.4483\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4142 - val_loss: 0.4475\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4139 - val_loss: 0.4470\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.4473\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4134 - val_loss: 0.4475\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4130 - val_loss: 0.4464\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.4468\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.4476\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4121 - val_loss: 0.4478\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4119 - val_loss: 0.4468\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4116 - val_loss: 0.4468\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4114 - val_loss: 0.4468\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4109 - val_loss: 0.4463\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4104 - val_loss: 0.4463\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4101 - val_loss: 0.4450\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 0.4458\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 0.4444\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4089 - val_loss: 0.4453\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4087 - val_loss: 0.4438\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4086 - val_loss: 0.4442\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.4453\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.4442\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4076 - val_loss: 0.4440\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4073 - val_loss: 0.4438\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4071 - val_loss: 0.4438\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.4431\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.4427\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4062 - val_loss: 0.4437\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4059 - val_loss: 0.4441\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4056 - val_loss: 0.4424\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4054 - val_loss: 0.4427\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4054 - val_loss: 0.4427\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4051 - val_loss: 0.4426\n",
      "121/121 [==============================] - 0s 707us/step - loss: 0.4214\n",
      "[CV]  learning_rate=0.002388469823418883, n_hidden=1, n_neurons=4, total=  28.8s\n",
      "[CV] learning_rate=0.002388469823418883, n_hidden=1, n_neurons=4 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1317 - val_loss: 0.9427\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7751 - val_loss: 0.7173\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6429 - val_loss: 0.6559\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5942 - val_loss: 0.6201\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5636 - val_loss: 0.5910\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5401 - val_loss: 0.5690\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5219 - val_loss: 0.5503\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5070 - val_loss: 0.5328\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4950 - val_loss: 0.5201\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4859 - val_loss: 0.5086\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4779 - val_loss: 0.5013\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4720 - val_loss: 0.4960\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4674 - val_loss: 0.4874\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4631 - val_loss: 0.4839\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4600 - val_loss: 0.4781\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4572 - val_loss: 0.4754\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4552 - val_loss: 0.4721\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4532 - val_loss: 0.4709\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4520 - val_loss: 0.4709\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4504 - val_loss: 0.4687\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4497 - val_loss: 0.4683\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4484 - val_loss: 0.4682\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4476 - val_loss: 0.4647\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4465 - val_loss: 0.4681\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4464 - val_loss: 0.4644\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4453 - val_loss: 0.4612\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4449 - val_loss: 0.4634\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4446 - val_loss: 0.4621\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4436 - val_loss: 0.4639\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4437 - val_loss: 0.4650\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4432 - val_loss: 0.4644\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4430 - val_loss: 0.4627\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4426 - val_loss: 0.4609\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4416 - val_loss: 0.4637\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4425 - val_loss: 0.4635\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4419 - val_loss: 0.4604\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4415 - val_loss: 0.4582\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4413 - val_loss: 0.4609\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4412 - val_loss: 0.4610\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4407 - val_loss: 0.4623\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4402 - val_loss: 0.4607\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4407 - val_loss: 0.4598\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4403 - val_loss: 0.4612\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4405 - val_loss: 0.4615\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4400 - val_loss: 0.4614\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4401 - val_loss: 0.4583\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4397 - val_loss: 0.4572\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4401 - val_loss: 0.4584\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4392 - val_loss: 0.4580\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4397 - val_loss: 0.4585\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4392 - val_loss: 0.4603\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4395 - val_loss: 0.4598\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4392 - val_loss: 0.4583\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4392 - val_loss: 0.4582\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4387 - val_loss: 0.4586\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4387 - val_loss: 0.4574\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4388 - val_loss: 0.4566\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4391 - val_loss: 0.4584\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4385 - val_loss: 0.4582\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4384 - val_loss: 0.4581\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4383 - val_loss: 0.4586\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4381 - val_loss: 0.4605\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4383 - val_loss: 0.4577\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4381 - val_loss: 0.4592\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4381 - val_loss: 0.4557\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4579\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4379 - val_loss: 0.4585\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4570\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4375 - val_loss: 0.4590\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4374 - val_loss: 0.4570\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4372 - val_loss: 0.4560\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4370 - val_loss: 0.4588\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4371 - val_loss: 0.4571\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4372 - val_loss: 0.4556\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4371 - val_loss: 0.4569\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4365 - val_loss: 0.4574\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4369 - val_loss: 0.4558\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4363 - val_loss: 0.4594\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4368 - val_loss: 0.4562\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4363 - val_loss: 0.4571\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4365 - val_loss: 0.4551\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4363 - val_loss: 0.4552\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4361 - val_loss: 0.4576\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4360 - val_loss: 0.4575\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4357 - val_loss: 0.4556\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4358 - val_loss: 0.4568\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4357 - val_loss: 0.4562\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4355 - val_loss: 0.4569\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4354 - val_loss: 0.4589\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4353 - val_loss: 0.4556\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4349 - val_loss: 0.4579\n",
      "121/121 [==============================] - 0s 815us/step - loss: 0.4239\n",
      "[CV]  learning_rate=0.002388469823418883, n_hidden=1, n_neurons=4, total=  26.3s\n",
      "[CV] learning_rate=0.02298924804076755, n_hidden=1, n_neurons=9 ......\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8382 - val_loss: 0.5397\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4988 - val_loss: 0.4946\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4414 - val_loss: 0.4659\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.4189 - val_loss: 0.4443\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4044 - val_loss: 0.4318\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3930 - val_loss: 0.4467\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3866 - val_loss: 0.4348\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3806 - val_loss: 0.4326\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3782 - val_loss: 0.4501\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3765 - val_loss: 0.4280\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3752 - val_loss: 0.4476\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3709 - val_loss: 0.4392\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3688 - val_loss: 0.4312\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3669 - val_loss: 0.4350\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3657 - val_loss: 0.4294\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3657 - val_loss: 0.4304\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3638 - val_loss: 0.4303\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.4345\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3615 - val_loss: 0.4309\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3621 - val_loss: 0.4310\n",
      "121/121 [==============================] - 0s 605us/step - loss: 0.3735\n",
      "[CV]  learning_rate=0.02298924804076755, n_hidden=1, n_neurons=9, total=   5.9s\n",
      "[CV] learning_rate=0.02298924804076755, n_hidden=1, n_neurons=9 ......\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8351 - val_loss: 0.5744\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5039 - val_loss: 0.5075\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4521 - val_loss: 0.4726\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4348 - val_loss: 0.4541\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4210 - val_loss: 0.4507\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4121 - val_loss: 0.4483\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.4390\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 0.4328\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3964 - val_loss: 0.4442\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3963 - val_loss: 0.4316\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3916 - val_loss: 0.4376\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3890 - val_loss: 0.4367\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3888 - val_loss: 0.4354\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3954 - val_loss: 0.4319\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3874 - val_loss: 0.4296\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3838 - val_loss: 0.4303\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3841 - val_loss: 0.4270\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3841 - val_loss: 0.4523\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3816 - val_loss: 0.4271\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3815 - val_loss: 0.4298\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3832 - val_loss: 0.4321\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3827 - val_loss: 0.4278\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3791 - val_loss: 0.4498\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3793 - val_loss: 0.4322\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3768 - val_loss: 0.4474\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.4316\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3761 - val_loss: 0.4328\n",
      "121/121 [==============================] - 0s 745us/step - loss: 0.3871\n",
      "[CV]  learning_rate=0.02298924804076755, n_hidden=1, n_neurons=9, total=   7.9s\n",
      "[CV] learning_rate=0.02298924804076755, n_hidden=1, n_neurons=9 ......\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8017 - val_loss: 0.5686\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4774 - val_loss: 0.4669\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4421 - val_loss: 0.4503\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.4519\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4194 - val_loss: 0.4458\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4114 - val_loss: 0.4631\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4098 - val_loss: 0.4501\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.4483\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4001 - val_loss: 0.4416\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4003 - val_loss: 0.4423\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3947 - val_loss: 0.4432\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.4571\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3902 - val_loss: 0.4454\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3868 - val_loss: 0.4511\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3890 - val_loss: 0.4433\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3847 - val_loss: 0.4387\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3831 - val_loss: 0.4343\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3867 - val_loss: 0.4549\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3806 - val_loss: 0.4465\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3765 - val_loss: 0.4483\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3833 - val_loss: 0.4536\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3933 - val_loss: 0.4524\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3844 - val_loss: 0.4628\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.4506\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3785 - val_loss: 0.4381\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.4349\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3767 - val_loss: 0.4378\n",
      "121/121 [==============================] - 0s 647us/step - loss: 0.3614\n",
      "[CV]  learning_rate=0.02298924804076755, n_hidden=1, n_neurons=9, total=   8.0s\n",
      "[CV] learning_rate=0.00032288937857243905, n_hidden=1, n_neurons=84 ..\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4996 - val_loss: 2.1970\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7579 - val_loss: 1.2906\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1594 - val_loss: 0.9739\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9255 - val_loss: 0.8459\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8188 - val_loss: 0.7829\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7623 - val_loss: 0.7461\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7282 - val_loss: 0.7219\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7049 - val_loss: 0.7037\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6873 - val_loss: 0.6895\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6727 - val_loss: 0.6773\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6599 - val_loss: 0.6666\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6485 - val_loss: 0.6571\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6380 - val_loss: 0.6481\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6283 - val_loss: 0.6402\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6191 - val_loss: 0.6323\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6103 - val_loss: 0.6248\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6021 - val_loss: 0.6178\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5941 - val_loss: 0.6110\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5864 - val_loss: 0.6047\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5791 - val_loss: 0.5982\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5722 - val_loss: 0.5923\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5654 - val_loss: 0.5867\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5590 - val_loss: 0.5811\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5527 - val_loss: 0.5755\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5469 - val_loss: 0.5704\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5411 - val_loss: 0.5656\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5356 - val_loss: 0.5609\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5303 - val_loss: 0.5565\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5252 - val_loss: 0.5520\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5203 - val_loss: 0.5478\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5157 - val_loss: 0.5438\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5112 - val_loss: 0.5402\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.5068 - val_loss: 0.5364\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5027 - val_loss: 0.5329\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4987 - val_loss: 0.5295\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4948 - val_loss: 0.5260\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4912 - val_loss: 0.5229\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4876 - val_loss: 0.5197\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4841 - val_loss: 0.5167\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4809 - val_loss: 0.5141\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4777 - val_loss: 0.5114\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4747 - val_loss: 0.5089\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4718 - val_loss: 0.5062\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4689 - val_loss: 0.5038\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4663 - val_loss: 0.5015\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4636 - val_loss: 0.4991\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4612 - val_loss: 0.4970\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4587 - val_loss: 0.4950\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4564 - val_loss: 0.4930\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4542 - val_loss: 0.4909\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4520 - val_loss: 0.4889\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4499 - val_loss: 0.4873\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4480 - val_loss: 0.4858\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4460 - val_loss: 0.4841\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4442 - val_loss: 0.4828\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4424 - val_loss: 0.4813\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4407 - val_loss: 0.4797\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 999us/step - loss: 0.4390 - val_loss: 0.4785\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4374 - val_loss: 0.4770\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4358 - val_loss: 0.4757\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4344 - val_loss: 0.4746\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4329 - val_loss: 0.4736\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4316 - val_loss: 0.4723\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4302 - val_loss: 0.4713\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4289 - val_loss: 0.4704\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4277 - val_loss: 0.4694\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.4685\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4253 - val_loss: 0.4677\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4242 - val_loss: 0.4666\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4231 - val_loss: 0.4655\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4221 - val_loss: 0.4649\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4211 - val_loss: 0.4642\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.4635\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4192 - val_loss: 0.4627\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4183 - val_loss: 0.4621\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4173 - val_loss: 0.4611\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4165 - val_loss: 0.4605\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4157 - val_loss: 0.4602\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4148 - val_loss: 0.4596\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4140 - val_loss: 0.4588\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4133 - val_loss: 0.4586\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4125 - val_loss: 0.4580\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.4574\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4110 - val_loss: 0.4568\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4104 - val_loss: 0.4561\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 0.4560\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4090 - val_loss: 0.4552\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4084 - val_loss: 0.4547\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4077 - val_loss: 0.4542\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.4071 - val_loss: 0.4539\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4065 - val_loss: 0.4531\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4059 - val_loss: 0.4527\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4053 - val_loss: 0.4524\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4047 - val_loss: 0.4518\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4042 - val_loss: 0.4515\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.4512\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4030 - val_loss: 0.4507\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4025 - val_loss: 0.4500\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4019 - val_loss: 0.4502\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4015 - val_loss: 0.4494\n",
      "121/121 [==============================] - 0s 600us/step - loss: 0.4123\n",
      "[CV]  learning_rate=0.00032288937857243905, n_hidden=1, n_neurons=84, total=  28.1s\n",
      "[CV] learning_rate=0.00032288937857243905, n_hidden=1, n_neurons=84 ..\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3208 - val_loss: 2.3668\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8815 - val_loss: 1.6426\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3241 - val_loss: 1.3357\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0724 - val_loss: 1.1688\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9396 - val_loss: 1.0588\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8607 - val_loss: 0.9809\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8100 - val_loss: 0.9230\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7750 - val_loss: 0.8760\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7491 - val_loss: 0.8388\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7284 - val_loss: 0.8070\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7108 - val_loss: 0.7801\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6954 - val_loss: 0.7556\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6815 - val_loss: 0.7344\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6688 - val_loss: 0.7159\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6569 - val_loss: 0.6987\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6458 - val_loss: 0.6839\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6352 - val_loss: 0.6698\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6252 - val_loss: 0.6569\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6157 - val_loss: 0.6450\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.6067 - val_loss: 0.6342\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5982 - val_loss: 0.6238\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5900 - val_loss: 0.6145\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5822 - val_loss: 0.6056\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5747 - val_loss: 0.5975\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5676 - val_loss: 0.5898\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5608 - val_loss: 0.5825\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5542 - val_loss: 0.5762\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5480 - val_loss: 0.5696\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5420 - val_loss: 0.5648\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5365 - val_loss: 0.5591\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5311 - val_loss: 0.5535\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5260 - val_loss: 0.5488\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5211 - val_loss: 0.5443\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5164 - val_loss: 0.5406\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5120 - val_loss: 0.5367\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5078 - val_loss: 0.5326\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5038 - val_loss: 0.5293\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5000 - val_loss: 0.5259\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4963 - val_loss: 0.5227\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4929 - val_loss: 0.5201\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4896 - val_loss: 0.5176\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4865 - val_loss: 0.5149\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4835 - val_loss: 0.5120\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4806 - val_loss: 0.5096\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4779 - val_loss: 0.5077\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4753 - val_loss: 0.5051\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4728 - val_loss: 0.5032\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4704 - val_loss: 0.5015\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4682 - val_loss: 0.4998\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4660 - val_loss: 0.4978\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4639 - val_loss: 0.4957\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4619 - val_loss: 0.4942\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4599 - val_loss: 0.4926\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4580 - val_loss: 0.4913\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4563 - val_loss: 0.4901\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4545 - val_loss: 0.4886\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4529 - val_loss: 0.4868\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4513 - val_loss: 0.4860\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4498 - val_loss: 0.4848\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4482 - val_loss: 0.4832\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4468 - val_loss: 0.4824\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4454 - val_loss: 0.4817\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4441 - val_loss: 0.4803\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4428 - val_loss: 0.4792\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4415 - val_loss: 0.4788\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4403 - val_loss: 0.4780\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4391 - val_loss: 0.4768\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4379 - val_loss: 0.4762\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4368 - val_loss: 0.4754\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4356 - val_loss: 0.4737\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4346 - val_loss: 0.4728\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4336 - val_loss: 0.4725\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4325 - val_loss: 0.4715\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4316 - val_loss: 0.4710\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4703\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4297 - val_loss: 0.4698\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4287 - val_loss: 0.4687\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4279 - val_loss: 0.4683\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4270 - val_loss: 0.4678\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.4671\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4253 - val_loss: 0.4667\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4245 - val_loss: 0.4661\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4236 - val_loss: 0.4659\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.4650\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4221 - val_loss: 0.4643\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4213 - val_loss: 0.4639\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4206 - val_loss: 0.4634\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.4629\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.4626\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4184 - val_loss: 0.4622\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4177 - val_loss: 0.4612\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4171 - val_loss: 0.4609\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4164 - val_loss: 0.4605\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4157 - val_loss: 0.4602\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.4598\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4144 - val_loss: 0.4595\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4138 - val_loss: 0.4589\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4132 - val_loss: 0.4581\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4126 - val_loss: 0.4580\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.4577\n",
      "121/121 [==============================] - 0s 659us/step - loss: 0.4188\n",
      "[CV]  learning_rate=0.00032288937857243905, n_hidden=1, n_neurons=84, total=  29.8s\n",
      "[CV] learning_rate=0.00032288937857243905, n_hidden=1, n_neurons=84 ..\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6048 - val_loss: 2.2565\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8714 - val_loss: 1.3470\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2445 - val_loss: 1.0087\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9832 - val_loss: 0.8697\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8703 - val_loss: 0.8067\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8160 - val_loss: 0.7733\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7851 - val_loss: 0.7515\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7643 - val_loss: 0.7347\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7484 - val_loss: 0.7215\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7346 - val_loss: 0.7092\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7222 - val_loss: 0.6981\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7111 - val_loss: 0.6879\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7005 - val_loss: 0.6787\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6907 - val_loss: 0.6702\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6812 - val_loss: 0.6623\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6722 - val_loss: 0.6540\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6637 - val_loss: 0.6470\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6554 - val_loss: 0.6400\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6474 - val_loss: 0.6328\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6401 - val_loss: 0.6266\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6327 - val_loss: 0.6205\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6256 - val_loss: 0.6146\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6189 - val_loss: 0.6094\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6121 - val_loss: 0.6046\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6057 - val_loss: 0.5991\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5995 - val_loss: 0.5941\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5934 - val_loss: 0.5885\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5878 - val_loss: 0.5843\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5820 - val_loss: 0.5800\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5767 - val_loss: 0.5755\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5712 - val_loss: 0.5708\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5662 - val_loss: 0.5670\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5610 - val_loss: 0.5627\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5563 - val_loss: 0.5594\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5516 - val_loss: 0.5559\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5471 - val_loss: 0.5527\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5427 - val_loss: 0.5496\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5383 - val_loss: 0.5459\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5343 - val_loss: 0.5425\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5303 - val_loss: 0.5393\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5264 - val_loss: 0.5369\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5228 - val_loss: 0.5342\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5191 - val_loss: 0.5310\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5157 - val_loss: 0.5286\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5122 - val_loss: 0.5258\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5091 - val_loss: 0.5231\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5059 - val_loss: 0.5214\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5028 - val_loss: 0.5188\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4998 - val_loss: 0.5165\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4969 - val_loss: 0.5143\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4941 - val_loss: 0.5121\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4914 - val_loss: 0.5101\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4888 - val_loss: 0.5083\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4862 - val_loss: 0.5067\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4837 - val_loss: 0.5051\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4813 - val_loss: 0.5036\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4790 - val_loss: 0.5013\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4768 - val_loss: 0.4997\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4746 - val_loss: 0.4981\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4724 - val_loss: 0.4959\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4704 - val_loss: 0.4951\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4684 - val_loss: 0.4939\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4666 - val_loss: 0.4922\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4646 - val_loss: 0.4906\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4629 - val_loss: 0.4896\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4611 - val_loss: 0.4885\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4594 - val_loss: 0.4867\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4578 - val_loss: 0.4856\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4562 - val_loss: 0.4850\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4546 - val_loss: 0.4833\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4531 - val_loss: 0.4818\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4517 - val_loss: 0.4810\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4503 - val_loss: 0.4799\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4489 - val_loss: 0.4794\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4476 - val_loss: 0.4787\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4463 - val_loss: 0.4776\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4450 - val_loss: 0.4763\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4437 - val_loss: 0.4751\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4426 - val_loss: 0.4747\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4413 - val_loss: 0.4743\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4403 - val_loss: 0.4733\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4392 - val_loss: 0.4721\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4380 - val_loss: 0.4713\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4371 - val_loss: 0.4705\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4359 - val_loss: 0.4696\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4349 - val_loss: 0.4691\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4340 - val_loss: 0.4684\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4330 - val_loss: 0.4675\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4321 - val_loss: 0.4672\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4312 - val_loss: 0.4662\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4303 - val_loss: 0.4653\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.4648\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4285 - val_loss: 0.4641\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4277 - val_loss: 0.4636\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4268 - val_loss: 0.4632\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.4626\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4253 - val_loss: 0.4621\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.4613\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4237 - val_loss: 0.4607\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4230 - val_loss: 0.4603\n",
      "121/121 [==============================] - 0s 706us/step - loss: 0.4247\n",
      "[CV]  learning_rate=0.00032288937857243905, n_hidden=1, n_neurons=84, total=  29.1s\n",
      "[CV] learning_rate=0.000910274101743386, n_hidden=3, n_neurons=71 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.5772 - val_loss: 1.1309\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9236 - val_loss: 0.7694\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7308 - val_loss: 0.6843\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6749 - val_loss: 0.6419\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6398 - val_loss: 0.6157\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6101 - val_loss: 0.5925\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5832 - val_loss: 0.5675\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5600 - val_loss: 0.5511\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5384 - val_loss: 0.5349\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5183 - val_loss: 0.5198\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5010 - val_loss: 0.5093\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4859 - val_loss: 0.5006\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4718 - val_loss: 0.4887\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4606 - val_loss: 0.4846\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4497 - val_loss: 0.4744\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4406 - val_loss: 0.4708\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4325 - val_loss: 0.4650\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4251 - val_loss: 0.4603\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.4583\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4130 - val_loss: 0.4515\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.4520\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4026 - val_loss: 0.4479\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3986 - val_loss: 0.4442\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.4420\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3908 - val_loss: 0.4404\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3873 - val_loss: 0.4375\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3847 - val_loss: 0.4357\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3810 - val_loss: 0.4343\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3786 - val_loss: 0.4321\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3761 - val_loss: 0.4306\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3733 - val_loss: 0.4279\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3714 - val_loss: 0.4277\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.4253\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3667 - val_loss: 0.4230\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3650 - val_loss: 0.4217\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3629 - val_loss: 0.4216\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3613 - val_loss: 0.4185\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3593 - val_loss: 0.4198\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3576 - val_loss: 0.4159\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.4170\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3544 - val_loss: 0.4169\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.4156\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.4147\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.4138\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.4139\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.4131\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.4115\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3450 - val_loss: 0.4122\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3432 - val_loss: 0.4097\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3422 - val_loss: 0.4096\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3413 - val_loss: 0.4125\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3400 - val_loss: 0.4086\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3392 - val_loss: 0.4085\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3381 - val_loss: 0.4089\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3363 - val_loss: 0.4091\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3363 - val_loss: 0.4064\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3348 - val_loss: 0.4073\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.4045\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3328 - val_loss: 0.4056\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3316 - val_loss: 0.4054\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3308 - val_loss: 0.4047\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3299 - val_loss: 0.4066\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.4053\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3283 - val_loss: 0.4041\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3277 - val_loss: 0.4036\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3273 - val_loss: 0.4040\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3261 - val_loss: 0.4041\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3255 - val_loss: 0.4034\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.4022\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3242 - val_loss: 0.4023\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3231 - val_loss: 0.4010\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3226 - val_loss: 0.4003\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3219 - val_loss: 0.4009\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3214 - val_loss: 0.4007\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3206 - val_loss: 0.4014\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3197 - val_loss: 0.4014\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.4005\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3186 - val_loss: 0.4004\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3184 - val_loss: 0.3987\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3176 - val_loss: 0.4009\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3168 - val_loss: 0.4022\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3166 - val_loss: 0.3986\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3162 - val_loss: 0.3989\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3153 - val_loss: 0.3978\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3150 - val_loss: 0.4006\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3140 - val_loss: 0.3982\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3141 - val_loss: 0.3973\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3134 - val_loss: 0.4001\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3131 - val_loss: 0.3967\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3125 - val_loss: 0.3975\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3123 - val_loss: 0.3993\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3116 - val_loss: 0.3966\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3110 - val_loss: 0.3980\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3108 - val_loss: 0.3971\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3100 - val_loss: 0.3958\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3090 - val_loss: 0.3959\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3089 - val_loss: 0.3957\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3089 - val_loss: 0.3969\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3079 - val_loss: 0.3967\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3080 - val_loss: 0.3950\n",
      "121/121 [==============================] - 0s 654us/step - loss: 0.3375\n",
      "[CV]  learning_rate=0.000910274101743386, n_hidden=3, n_neurons=71, total=  33.7s\n",
      "[CV] learning_rate=0.000910274101743386, n_hidden=3, n_neurons=71 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.1156 - val_loss: 1.0787\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8922 - val_loss: 0.8211\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7417 - val_loss: 0.7275\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6819 - val_loss: 0.6797\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6446 - val_loss: 0.6467\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6157 - val_loss: 0.6243\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5908 - val_loss: 0.6009\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5686 - val_loss: 0.5815\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5486 - val_loss: 0.5657\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5306 - val_loss: 0.5513\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5139 - val_loss: 0.5378\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4990 - val_loss: 0.5249\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4853 - val_loss: 0.5131\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4731 - val_loss: 0.5081\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4617 - val_loss: 0.4947\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4514 - val_loss: 0.4890\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4421 - val_loss: 0.4798\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4340 - val_loss: 0.4751\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.4675\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4195 - val_loss: 0.4635\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.4580\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4077 - val_loss: 0.4539\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4029 - val_loss: 0.4515\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3984 - val_loss: 0.4465\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3944 - val_loss: 0.4430\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3906 - val_loss: 0.4409\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3870 - val_loss: 0.4389\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3839 - val_loss: 0.4366\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3805 - val_loss: 0.4374\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3785 - val_loss: 0.4335\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3756 - val_loss: 0.4300\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.4300\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3710 - val_loss: 0.4293\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3689 - val_loss: 0.4280\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3668 - val_loss: 0.4267\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3648 - val_loss: 0.4257\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3633 - val_loss: 0.4253\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3614 - val_loss: 0.4257\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3600 - val_loss: 0.4216\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3586 - val_loss: 0.4225\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3569 - val_loss: 0.4221\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3554 - val_loss: 0.4234\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.4207\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3530 - val_loss: 0.4204\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3517 - val_loss: 0.4220\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3506 - val_loss: 0.4185\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.4178\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.4178\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.4179\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.4173\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.4195\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3443 - val_loss: 0.4166\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3434 - val_loss: 0.4145\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3422 - val_loss: 0.4157\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3415 - val_loss: 0.4147\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3406 - val_loss: 0.4133\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3396 - val_loss: 0.4127\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3391 - val_loss: 0.4130\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3382 - val_loss: 0.4124\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3371 - val_loss: 0.4122\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3364 - val_loss: 0.4119\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3358 - val_loss: 0.4125\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.4106\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3344 - val_loss: 0.4108\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3334 - val_loss: 0.4115\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3328 - val_loss: 0.4120\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3323 - val_loss: 0.4104\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3313 - val_loss: 0.4094\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3307 - val_loss: 0.4083\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3298 - val_loss: 0.4081\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3294 - val_loss: 0.4076\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3288 - val_loss: 0.4075\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3279 - val_loss: 0.4065\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3274 - val_loss: 0.4071\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3269 - val_loss: 0.4063\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3259 - val_loss: 0.4064\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.4063\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3248 - val_loss: 0.4071\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3244 - val_loss: 0.4054\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3234 - val_loss: 0.4062\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3230 - val_loss: 0.4054\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3224 - val_loss: 0.4041\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3215 - val_loss: 0.4052\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3213 - val_loss: 0.4031\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3206 - val_loss: 0.4030\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.4027\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3195 - val_loss: 0.4032\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3188 - val_loss: 0.4039\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3185 - val_loss: 0.4023\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3179 - val_loss: 0.4018\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3174 - val_loss: 0.4019\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3168 - val_loss: 0.4006\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3163 - val_loss: 0.4008\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.3988\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3149 - val_loss: 0.3997\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3148 - val_loss: 0.3998\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3139 - val_loss: 0.3978\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3137 - val_loss: 0.3977\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3133 - val_loss: 0.3981\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3127 - val_loss: 0.3975\n",
      "121/121 [==============================] - 0s 714us/step - loss: 0.3365\n",
      "[CV]  learning_rate=0.000910274101743386, n_hidden=3, n_neurons=71, total=  33.5s\n",
      "[CV] learning_rate=0.000910274101743386, n_hidden=3, n_neurons=71 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.2865 - val_loss: 0.9960\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8907 - val_loss: 0.7579\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7291 - val_loss: 0.6827\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6615 - val_loss: 0.6403\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6158 - val_loss: 0.6076\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5791 - val_loss: 0.5800\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5480 - val_loss: 0.5613\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5223 - val_loss: 0.5383\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5000 - val_loss: 0.5229\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4814 - val_loss: 0.5103\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4650 - val_loss: 0.4982\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4514 - val_loss: 0.4872\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4404 - val_loss: 0.4794\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4309 - val_loss: 0.4749\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.4671\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4161 - val_loss: 0.4610\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4105 - val_loss: 0.4583\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4054 - val_loss: 0.4540\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4016 - val_loss: 0.4515\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3979 - val_loss: 0.4489\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3946 - val_loss: 0.4463\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3914 - val_loss: 0.4462\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3889 - val_loss: 0.4438\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.4450\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3840 - val_loss: 0.4416\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.4388\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3799 - val_loss: 0.4378\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3778 - val_loss: 0.4370\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3761 - val_loss: 0.4384\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3748 - val_loss: 0.4357\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.4346\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3716 - val_loss: 0.4350\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3700 - val_loss: 0.4326\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.4357\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.4326\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3661 - val_loss: 0.4303\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3654 - val_loss: 0.4297\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.4309\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3627 - val_loss: 0.4309\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3618 - val_loss: 0.4301\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.4296\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3597 - val_loss: 0.4294\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3586 - val_loss: 0.4277\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3579 - val_loss: 0.4266\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3568 - val_loss: 0.4297\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.4259\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3548 - val_loss: 0.4249\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3542 - val_loss: 0.4248\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.4252\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.4239\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.4264\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3506 - val_loss: 0.4236\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.4229\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.4233\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.4232\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.4226\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.4228\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.4217\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3450 - val_loss: 0.4211\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3441 - val_loss: 0.4216\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3437 - val_loss: 0.4218\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3428 - val_loss: 0.4217\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3424 - val_loss: 0.4208\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3418 - val_loss: 0.4207\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3411 - val_loss: 0.4205\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3402 - val_loss: 0.4212\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3397 - val_loss: 0.4195\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3392 - val_loss: 0.4194\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3384 - val_loss: 0.4197\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3378 - val_loss: 0.4188\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3372 - val_loss: 0.4168\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3366 - val_loss: 0.4183\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3358 - val_loss: 0.4190\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.4179\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.4167\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3339 - val_loss: 0.4175\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3339 - val_loss: 0.4168\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3331 - val_loss: 0.4165\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3325 - val_loss: 0.4172\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3319 - val_loss: 0.4174\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3315 - val_loss: 0.4151\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3310 - val_loss: 0.4164\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3304 - val_loss: 0.4174\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3299 - val_loss: 0.4142\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.4150\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3290 - val_loss: 0.4141\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3282 - val_loss: 0.4149\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3278 - val_loss: 0.4142\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3274 - val_loss: 0.4149\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3268 - val_loss: 0.4147\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3262 - val_loss: 0.4162\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3259 - val_loss: 0.4132\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3252 - val_loss: 0.4122\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3249 - val_loss: 0.4125\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3241 - val_loss: 0.4140\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 0.4136\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3235 - val_loss: 0.4111\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3230 - val_loss: 0.4120\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.4115\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3224 - val_loss: 0.4127\n",
      "121/121 [==============================] - 0s 811us/step - loss: 0.3300\n",
      "[CV]  learning_rate=0.000910274101743386, n_hidden=3, n_neurons=71, total=  33.3s\n",
      "[CV] learning_rate=0.0049786840924071745, n_hidden=3, n_neurons=47 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2010 - val_loss: 0.5842\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 0.4987\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4452 - val_loss: 0.4572\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4062 - val_loss: 0.4428\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3869 - val_loss: 0.4364\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3756 - val_loss: 0.4329\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3686 - val_loss: 0.4349\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.4262\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.4366\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.4235\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.4277\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3437 - val_loss: 0.4269\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3396 - val_loss: 0.4222\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3388 - val_loss: 0.4176\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3348 - val_loss: 0.4161\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3331 - val_loss: 0.4178\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3308 - val_loss: 0.4150\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3295 - val_loss: 0.4196\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3263 - val_loss: 0.4113\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3285 - val_loss: 0.4178\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3249 - val_loss: 0.4135\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3236 - val_loss: 0.4157\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3203 - val_loss: 0.4108\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3230 - val_loss: 0.4263\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3205 - val_loss: 0.4218\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3162 - val_loss: 0.4127\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3160 - val_loss: 0.4117\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3153 - val_loss: 0.4147\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3123 - val_loss: 0.4063\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3122 - val_loss: 0.4133\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3125 - val_loss: 0.4064\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3093 - val_loss: 0.4073\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3073 - val_loss: 0.4042\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3056 - val_loss: 0.4130\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3053 - val_loss: 0.4088\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3048 - val_loss: 0.4038\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3036 - val_loss: 0.4165\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3044 - val_loss: 0.4079\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3017 - val_loss: 0.4062\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2999 - val_loss: 0.4019\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2983 - val_loss: 0.4066\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2966 - val_loss: 0.4033\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2963 - val_loss: 0.4053\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2940 - val_loss: 0.4084\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2952 - val_loss: 0.4039\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2944 - val_loss: 0.3971\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2915 - val_loss: 0.3954\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2909 - val_loss: 0.3967\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2896 - val_loss: 0.3983\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2898 - val_loss: 0.4032\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2872 - val_loss: 0.4343\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2865 - val_loss: 0.4019\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2859 - val_loss: 0.3985\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2839 - val_loss: 0.4144\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2839 - val_loss: 0.4018\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2832 - val_loss: 0.3992\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2817 - val_loss: 0.4022\n",
      "121/121 [==============================] - 0s 728us/step - loss: 0.3186\n",
      "[CV]  learning_rate=0.0049786840924071745, n_hidden=3, n_neurons=47, total=  18.3s\n",
      "[CV] learning_rate=0.0049786840924071745, n_hidden=3, n_neurons=47 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0931 - val_loss: 0.6097\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5542 - val_loss: 0.5215\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4615 - val_loss: 0.4748\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.4519\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3973 - val_loss: 0.4404\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3842 - val_loss: 0.4361\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.4270\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3687 - val_loss: 0.4210\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3620 - val_loss: 0.4234\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3576 - val_loss: 0.4153\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3520 - val_loss: 0.4154\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.4151\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3445 - val_loss: 0.4169\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3426 - val_loss: 0.4146\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3397 - val_loss: 0.4095\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3359 - val_loss: 0.4065\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3342 - val_loss: 0.4051\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3323 - val_loss: 0.4065\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.4042\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3271 - val_loss: 0.4069\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.4032\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3231 - val_loss: 0.4020\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3208 - val_loss: 0.4053\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3197 - val_loss: 0.4054\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3171 - val_loss: 0.4121\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3151 - val_loss: 0.3997\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3135 - val_loss: 0.4030\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3122 - val_loss: 0.4033\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3114 - val_loss: 0.3988\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3111 - val_loss: 0.4024\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3079 - val_loss: 0.4008\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3065 - val_loss: 0.4057\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3056 - val_loss: 0.4010\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3036 - val_loss: 0.4017\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3027 - val_loss: 0.3981\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3017 - val_loss: 0.3997\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3008 - val_loss: 0.3907\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2991 - val_loss: 0.4098\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2981 - val_loss: 0.3901\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2967 - val_loss: 0.3970\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2949 - val_loss: 0.4034\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2941 - val_loss: 0.3951\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2934 - val_loss: 0.3928\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2923 - val_loss: 0.4081\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2912 - val_loss: 0.4056\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2910 - val_loss: 0.3891\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2897 - val_loss: 0.3890\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2889 - val_loss: 0.3927\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2870 - val_loss: 0.4012\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2884 - val_loss: 0.4003\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2854 - val_loss: 0.4202\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2859 - val_loss: 0.3927\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2837 - val_loss: 0.3930\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2838 - val_loss: 0.3983\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2825 - val_loss: 0.3933\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2823 - val_loss: 0.3953\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2802 - val_loss: 0.3971\n",
      "121/121 [==============================] - 0s 720us/step - loss: 0.3123\n",
      "[CV]  learning_rate=0.0049786840924071745, n_hidden=3, n_neurons=47, total=  18.5s\n",
      "[CV] learning_rate=0.0049786840924071745, n_hidden=3, n_neurons=47 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9812 - val_loss: 0.5780\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4935 - val_loss: 0.4910\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4320 - val_loss: 0.4614\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4083 - val_loss: 0.4606\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3998 - val_loss: 0.4469\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3848 - val_loss: 0.4452\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.4313\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3701 - val_loss: 0.4299\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3654 - val_loss: 0.4283\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3629 - val_loss: 0.4264\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.4280\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.4236\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 0.4257\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.4239\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.4244\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3432 - val_loss: 0.4169\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3429 - val_loss: 0.4152\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3394 - val_loss: 0.4234\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3372 - val_loss: 0.4182\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.4171\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3342 - val_loss: 0.4146\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3334 - val_loss: 0.4162\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3301 - val_loss: 0.4186\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3287 - val_loss: 0.4127\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3269 - val_loss: 0.4155\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3253 - val_loss: 0.4051\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3224 - val_loss: 0.4133\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3237 - val_loss: 0.4152\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3293 - val_loss: 0.4072\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.4169\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3180 - val_loss: 0.4047\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3163 - val_loss: 0.4066\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3159 - val_loss: 0.4090\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3139 - val_loss: 0.4016\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3138 - val_loss: 0.4203\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3107 - val_loss: 0.4107\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3108 - val_loss: 0.4120\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3120 - val_loss: 0.4142\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3142 - val_loss: 0.4022\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3090 - val_loss: 0.4049\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3074 - val_loss: 0.4101\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3049 - val_loss: 0.4021\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3060 - val_loss: 0.4074\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3052 - val_loss: 0.4090\n",
      "121/121 [==============================] - 0s 768us/step - loss: 0.3095\n",
      "[CV]  learning_rate=0.0049786840924071745, n_hidden=3, n_neurons=47, total=  15.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 10.8min finished\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7fae8af40be0>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-a85193da9d63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m rnd_search_cv.fit(X_train_scaled, y_train, epochs=100,\n\u001b[0m\u001b[1;32m     13\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                   callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;31m# we clone again after setting params in case some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;31m# of the params are estimators as well.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[1;32m    762\u001b[0m                 **self.best_params_))\n\u001b[1;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[1;32m     97\u001b[0m                                \u001b[0;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                                (estimator, name))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7fae8af40be0>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\":[0,1,2,3],\n",
    "    \"n_neurons\":np.arange(1,100),\n",
    "    \"learning_rate\":reciprocal(3e-4,3e-2),\n",
    "}\n",
    "\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train_scaled, y_train, epochs=100,\n",
    "                  validation_data=(X_valid_scaled, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0051747964719537, 'n_hidden': 3, 'n_neurons': 55}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.30924926201502484"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
